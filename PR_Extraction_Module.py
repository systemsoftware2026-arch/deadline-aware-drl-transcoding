import numpy as np
import pandas as pd
import random
import gymnasium as gym
from gymnasium import spaces
from sb3_contrib import MaskablePPO
from stable_baselines3.common.vec_env import SubprocVecEnv
from sb3_contrib.common.wrappers import ActionMasker
import torch
import os
import csv
import time
import matplotlib.pyplot as plt
import json, datetime
from sklearn.model_selection import GroupShuffleSplit
from sklearn.ensemble import RandomForestRegressor

from tqdm import tqdm


from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import DummyVecEnv  # âœ… ì¶”ê°€
from stable_baselines3.common.vec_env import VecNormalize  # âœ… ì¶”ê°€

import multiprocessing

# âœ… ìŠ¤ië ˆë“œ ìˆ˜ ì„¤ì • (ê°•ì œ ì œí•œí•  ê²½ìš°)
torch.set_num_threads(8)  # ë˜ëŠ” multiprocessing.cpu_count() // 2
# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
# âœ… ë©€í‹° í™˜ê²½ ìˆ˜ ìë™ ì„¤ì • (ì„ íƒì‚¬í•­)
#num_envs = min(8, multiprocessing.cpu_count() // 2)
num_envs = 4

KNN = 20

TRAINING = False
TESTING = not TRAINING  # TESTING ëª¨ë“œëŠ” TRAININGì´ ì•„ë‹ ë•Œ í™œì„±í™”
#MODEL_PATH = "model/default/pred_base.zip"
#MODEL_PATH = "model/default/default2.zip"
#MODEL_PATH = "model/noisemodel/hetero_gaussian.zip"
#MODEL_PATH = rf"model/dataset9/knn_{KNN}.zip"

MODEL_PATHS = {
    "DRL": "model/default/default2.zip",
    "Baseline_DRL": "model/default/pred_base.zip"
}

# ì‹¤í–‰ option  íŠ¸ë ˆì´ë‹ íƒ€ì„ ìŠ¤í… ìˆ˜ì™€ íƒ€ì„ ìŠ¬ë¡¯ ìš©ëŸ‰ í¬ê¸°  
 
TOTAL_STEPS = 1_000_000
#TOTAL_STEPS = 800_000
SIZE_FACTOR = 0.3


# FINETUNE ê³¼ì •ì´ ì“°ì¼ ê²½ìš° í•„ìš”í•¨ 
FINETUNE = False       # â† ì¶”ê°€ ë¯¸ì„¸ í•™ìŠµì„ í• ì§€ ì—¬ë¶€
FINETUNE_STEPS = 200_000 # â† 10~30ë§Œ ìŠ¤í… ì •ë„ ê¶Œì¥

# ì¸ê¸°ë„ ë³€í™” ëª¨ë¸ ì¸ì ì‹¤í—˜ ì‹œ ë‹¤ì–‘í•œ ì¸ì ë°”ê¿€ ìˆ˜ ìˆìŒ 
VIDEO_TRAINING_MODEL  = "hetero_gaussian"   #  "gaussian"  | "hetero_gaussian" | "hetero_gaussian_head" | "hetero_gaussian_tail" | "none" | "real_dataset_train" | "real_dataset_test"
VER_TRAINING_MODEL = "gaussian"  # "gaussian"   # "dirichlet" | "gaussian" | "none"
VIDEO_PARAM = 0.05  # SIGMA   0.02 ~ 0.05

HETERO_BETA = 0.4
VER_PARAM = 0.02 # SIGMA   0.02 ~ 0.05


ZIPF_PARAMETER = 0.791
#ZIPF_PARAMETER = 1.0
SLOT_CONCENTRATION = 4
FIXED_ZIPF = 1   # 1 or 0 

# NOT USED 
VIDEO_TAU = 1.5 
VIDEO_LAMBDA = 0.3


class TransEnv(gym.Env):
    def __init__(self,
                 video_noise_model="gaussian",   
                 video_noise_param=0.05,            # gaussian
                 video_tau=0.10,                    # Not used
                 video_lambda=0.40,                 # Not used
                 # ë²„ì „ ë…¸ì´ì¦ˆ
                 version_noise_model="gaussian",   # "dirichlet" | "gaussian" | "none"
                 version_noise_param=0.05,          # ê¸°ë³¸ ê°•ë„
                 seed=None):
        super(TransEnv, self).__init__()

        self.video_noise_model   = video_noise_model
        self.video_noise_param   = float(video_noise_param)
        self.video_tau           = float(video_tau)
        self.video_lambda        = float(video_lambda)
        self.version_noise_model = version_noise_model
        self.version_noise_param = float(version_noise_param)

        #self.n_slots = 100  # 10 X 10 
        self.n_deadline = 10
        #self.n_servers = 10
        self.n_servers = 3
        self.n_slots = self.n_deadline * self.n_servers
        #self.n_videos = 1000
        self.n_videos = 174
        self.n_ver = 7
        self.episode_count = 0

        self.bitrates = np.array([0.3, 0.7, 1.5, 2.5, 5.0, 8.0, 12.0], dtype=np.float32)
        self.current_video = 0
        self.time_used = np.zeros(self.n_slots, dtype=np.float32)
        self.slot_groups = np.arange(self.n_slots, dtype=np.int32) % self.n_deadline  # âœ… ì¶”ê°€

        self.allocation_dict = {s: [] for s in range(self.n_slots)}





        csv_path = "transcoding_dataset_final.csv"     # CSV ê²½ë¡œ
        uniform_length_sec = 30.0                     # ë¹„ë””ì˜¤ ê¸¸ì´
        uniform_bitrate_kbps = 8000.0                  # ë¹„ë””ì˜¤ ë¹„íŠ¸ë ˆì´íŠ¸

         # (1) ëª¨ë¸ í•™ìŠµ(í•œ ë²ˆë§Œ) & (2) ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        self._m_time = None   # RTF ëª¨ë¸
        self._m_vmaf = None   # VMAF ëª¨ë¸
        self._train_models_from_csv(csv_path)

        # resolutions: ë°˜ë“œì‹œ CSVì™€ ì¼ì¹˜í•´ì•¼ í•¨
        self._resolutions = np.array([144, 240, 288, 360, 480, 720, 1080], dtype=np.int32)

        # ì „ ë¹„ë””ì˜¤ ë™ì¼ ì…ë ¥ â†’ ê¸¸ì´/ë¹„íŠ¸ë ˆì´íŠ¸ ë²¡í„° ìƒì„±
        #video_lengths = np.full(self.n_videos, uniform_length_sec, dtype=np.float64)
        #bitrates_kbps = np.full(self.n_videos, uniform_bitrate_kbps, dtype=np.float64)

        # length: 300 ~ 600ì´ˆ
        video_lengths = np.random.uniform(
            low=uniform_length_sec,
            high=uniform_length_sec,
            size=self.n_videos
        )

        self.video_length = video_lengths

        # bitrate: 8000 ~ 10000 kbps
        bitrates_kbps = np.random.uniform(
            low=uniform_bitrate_kbps,
            high=uniform_bitrate_kbps + 2000,
            size=self.n_videos
        )
        

        self.time_size, self.vmaf = self._predict_time_vmaf_matrices(video_lengths, bitrates_kbps)
        # ë°˜ì˜¬ë¦¼(ì˜µì…˜)
        self.time_size = np.round(self.time_size, 3).astype(np.float32)
        self.vmaf = np.round(self.vmaf, 2).astype(np.float32)

        total_time_size = np.sum(self.time_size)
        total_time_budget = total_time_size * SIZE_FACTOR
        self.time_limit = np.full(self.n_slots, total_time_budget / self.n_slots, dtype=np.float32)  # (n_slots,)
        
        #print(self.time_size)
        #print(self.vmaf)
        #exit()
        
        
        # ë°ë“œë¼ì¸ (ê· ë“± ë¶„ë°° ëŒ€ì‹ ) ê°€ìš°ì‹œì•ˆ ë¶„í¬ ê¸°ë°˜ ëœë¤ ë°°ì •
        mu = self.n_deadline / 2     # í‰ê·  (ì˜ˆ: 5)
        sigma = self.n_deadline / 3  # í‘œì¤€í¸ì°¨ (ì˜ˆ: 3.3)
        raw = np.random.normal(mu, sigma, self.n_videos)
        # ì‹¤ìˆ˜ â†’ ì •ìˆ˜ ë³€í™˜ í›„ ë²”ìœ„ ì œí•œ
        self.deadline = np.clip(raw.astype(int), 0, self.n_deadline - 1)
        
        # Action Space
        self.n_combos = 64     # 6ë¹„íŠ¸ â†’ ver1..6 ì¤‘ ì €ì¥ ì¡°í•©, ver0ì€ í•­ìƒ í¬í•¨
        #self.n_slots  = 100
        self.n_slots  = self.n_deadline * self.n_servers
        self.action_space = spaces.Discrete(self.n_combos)

        state_dim = self.n_combos * 3 + 13 + 1

        self.observation_space = spaces.Box(
            low=0, high=1,
            shape=(state_dim,),
            dtype=np.float32
        )

        # â”€â”€ add in __init__ (or as class fields)
        self._cached_combo_vid = -1
        self._combo_sizes = np.zeros(self.n_combos, dtype=np.float32)

        if seed is not None:
            np.random.seed(seed)

        self.reset()


    @staticmethod
    def _make_features(df_like: pd.DataFrame) -> pd.DataFrame:
        # RTF ëª¨ë¸: ê¸¸ì´/BR/í•´ìƒë„ ëª¨ë‘ ì‚¬ìš©
        return pd.DataFrame({
            "log_len": np.log1p(df_like["video_length_sec"].astype(float)),
            "log_br":  np.log1p(df_like["original_bitrate_kbps"].astype(float)),
            "log_res": np.log1p(df_like["target_resolution"].astype(float)),
        })

    # ---------- ë‚´ë¶€: CSVë¡œë¶€í„° ëª¨ë¸ 1íšŒ í•™ìŠµ ----------
    def _train_models_from_csv(self, csv_path: str, random_state: int = 42):
        if (self._m_time is not None) and (self._m_vmaf is not None):
            return

        df = pd.read_csv(csv_path)
        # ì•ˆì „ ë³´ì •
        df = df.dropna(subset=[
            "video_id","video_length_sec","original_bitrate_kbps","target_resolution",
            "transcoding_time_sec","vmaf_score"
        ]).copy()
        df["vmaf_score"] = df["vmaf_score"].clip(0, 100)
        df["rtf"] = df["transcoding_time_sec"] / df["video_length_sec"].replace(0, np.nan)

        # íŠ¹ì§•
        X_all = self._make_features(df)
        y_rtf  = df["rtf"].values
        y_vmaf = df["vmaf_score"].values

        # video_id ê¸°ì¤€ ê·¸ë£¹ ë¶„í• (ëˆ„ìˆ˜ ë°©ì§€)
        gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)
        train_idx, _ = next(gss.split(X_all, groups=df["video_id"]))

        X_tr = X_all.iloc[train_idx]
        y_rtf_tr  = y_rtf[train_idx]
        y_vmaf_tr = y_vmaf[train_idx]

        # ëª¨ë¸ í•™ìŠµ
        m_time = RandomForestRegressor(
            n_estimators=300, min_samples_leaf=3, n_jobs=-1, random_state=random_state
        )
        m_vmaf = RandomForestRegressor(
            n_estimators=300, min_samples_leaf=3, n_jobs=-1, random_state=random_state
        )
        m_time.fit(X_tr, y_rtf_tr)
        # VMAFëŠ” ê¸¸ì´ì— ì˜ì¡´ì‹œí‚¤ì§€ ì•ŠìŒ(ê¸¸ì´ ì œì™¸)
        m_vmaf.fit(X_tr[["log_br","log_res"]], y_vmaf_tr)

        self._m_time = m_time
        self._m_vmaf = m_vmaf

    # ---------- ë‚´ë¶€: (ë¹„ë””ì˜¤Ã—ë²„ì „) ë§¤íŠ¸ë¦­ìŠ¤ ì˜ˆì¸¡ ----------
    def _predict_time_vmaf_matrices(self, video_lengths_sec: np.ndarray, bitrates_kbps: np.ndarray):
        """
        ì…ë ¥:  length[k], bitrate[k]  (k=0..n_videos-1), ì „ ë¹„ë””ì˜¤ ë™ì¼ ê°’ë„ OK
        ì¶œë ¥: time_size[n_videos, n_ver], vmaf[n_videos, n_ver]
        """
        assert self._m_time is not None and self._m_vmaf is not None, "Models must be trained first."
        assert video_lengths_sec.shape[0] == self.n_videos
        assert bitrates_kbps.shape[0] == self.n_videos

        # (ë¹„ë””ì˜¤Ã—í•´ìƒë„) ë°°ì¹˜ êµ¬ì„±
        vid_idx = np.repeat(np.arange(self.n_videos), self.n_ver)
        res_tile = np.tile(self._resolutions, self.n_videos)

        df_in = pd.DataFrame({
            "video_length_sec": video_lengths_sec[vid_idx],
            "original_bitrate_kbps": bitrates_kbps[vid_idx],
            "target_resolution": res_tile
        })
        X = self._make_features(df_in)

        # RTF â†’ ì‹œê°„(ì´ˆ)
        rtf_pred = self._m_time.predict(X.values)
        time_pred = rtf_pred * df_in["video_length_sec"].values

        # VMAF(0~100)
        vmaf_pred = self._m_vmaf.predict(X[["log_br","log_res"]].values)
        vmaf_pred = np.clip(vmaf_pred, 0, 100)

        # (n_videos, n_ver)ë¡œ ì¬ë°°ì—´
        time_mat = time_pred.reshape(self.n_videos, self.n_ver)
        vmaf_mat = vmaf_pred.reshape(self.n_videos, self.n_ver)
        return time_mat, vmaf_mat
    


    def get_HUF_value(self):
        return self.HUF_value

    def get_HUTF_value(self):
        return self.HUTF_value

    def get_HUF_strict_value(self):
        return self.HUF_strict_value

    def get_HUTF_strict_value(self):
        return self.HUTF_strict_value

    def get_MCKP_predict_value(self):
        return self.MCKP_predict_value

    def get_MCKP_true_value(self):
        return self.MCKP_true_value

    def get_PPO_value(self):
        return self.PPO_value



    def _resample_deadlines(self):
        mu = self.n_deadline / 2
        sigma = self.n_deadline / 3
        raw = np.random.normal(mu, sigma, self.n_videos)
        self.deadline = np.clip(raw.astype(int), 0, self.n_deadline - 1)    

    def _combo_size_vecs(self, vid: int):
        """í˜„ì¬ ë¹„ë””ì˜¤ vidì— ëŒ€í•œ (total_size, delta_same) ê°ê° 64ì°¨ì›, ë‘˜ ë‹¤ [0,1] ì •ê·œí™”."""
        self._ensure_combo_sizes(vid)
        size0 = float(self.time_size[vid, 0])

        total = self._combo_sizes.copy()                           # ì ˆëŒ€ í¬ê¸°
        delta_same = np.maximum(total - size0, 0.0)                # in-place ì¦ë¶„

        # ê°„ë‹¨ ì •ê·œí™”(ê° ë²¡í„°ì˜ per-video maxë¡œ ë‚˜ëˆ”; 0 division ë°©ì§€)
        total_norm = total / (total.max() + 1e-6)
        delta_norm = delta_same / (delta_same.max() + 1e-6)

        return total_norm.astype(np.float32), delta_norm.astype(np.float32)

    def _ensure_combo_sizes(self, vid: int) -> None:
        """í˜„ì¬ ë¹„ë””ì˜¤ vidì— ëŒ€í•´ ì½¤ë³´(64ê°œ) ì´ ìš©ëŸ‰ì„ ìºì‹œ. vid ë°”ë€” ë•Œë§Œ ê°±ì‹ ."""
        if self._cached_combo_vid == vid:
            return
        sizes = np.zeros(self.n_combos, dtype=np.float32)
        for c in range(self.n_combos):
            vs = self._combo_to_versions(c)  # í•­ìƒ ver0 í¬í•¨
            sizes[c] = float(np.sum(self.time_size[vid, vs]))
        self._combo_sizes[:] = sizes
        self._cached_combo_vid = vid

    def _decode_action(self, a):
        combo = a // self.n_slots      # 0..63
        slot  = a %  self.n_slots      # 0..99
        return combo, slot

    def _combo_to_versions(self, combo_idx):
        # ver0ì€ í•­ìƒ í¬í•¨, 6ë¹„íŠ¸ëŠ” ver1..6
        versions = [0]
        for b in range(6):
            if (combo_idx >> b) & 1:
                versions.append(b + 1)
        return sorted(versions)
    
    def _combo_fallback_qoe_vec(self, vid, use="pred"):
        # ì¡°í•©ë³„ë¡œ: ì €ì¥ì„¸íŠ¸(í•­ìƒ ver0 í¬í•¨) ê¸°ì¤€ fallback QoE ê³„ì‚°
        dist = self.pred_popularity if use=="pred" else self.true_popularity
        out = np.zeros(self.n_combos, dtype=np.float32)
        for c in range(self.n_combos):
            saved = self._combo_to_versions(c)
            # fallback ì‚¬ìš©ëœ ìµœì¢… ì œê³µ ë²„ì „ ë§¤í•‘
            fb = []
            for ver in range(self.n_ver):
                if ver in saved:
                    fb.append(ver)
                else:
                    lowers = [v for v in saved if v < ver]
                    fb.append(max(lowers) if lowers else 0)
            out[c] = float(np.sum(dist[vid] * self.vmaf[vid, fb]))
        # ì •ê·œí™”(ì„ íƒ): 0~1ë¡œ ìŠ¤ì¼€ì¼
        # mx = (self.vmaf[vid].max() + 1e-6)
        return out / 10.0

    def _versions_of_combo(self, combo_idx: int):
        vs = [0]
        for b in range(6):
            if (combo_idx >> b) & 1:
                vs.append(b + 1)
        return sorted(vs)

    def _combo_size(self, vid: int, combo_idx: int) -> float:
        vs = self._versions_of_combo(combo_idx)
        return float(np.sum(self.time_size[vid, vs]))

    def _leftover_after(self, slot: int, add_size: float) -> float:
        return float(self.time_limit[slot] - (self.time_used[slot] + add_size))

    def _pick_slot_for_combo(self, v: int, combo_idx: int):
        """
        ì •ì±…(ì—…ë°ì´íŠ¸):
        1) in-place(ì•µì»¤ ìœ ì§€): old ì—ì„œ í•„ìš”í•œ ì¦ë¶„ ì—¬ìœ  ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ
        2) ê°™ì€ ë°ë“œë¼ì¸ ì°½ì—ì„œ 'ê°€ê¹Œìš´ ê·¸ë£¹ë¶€í„°'(d, d-1, ..., lo) ê·¸ë£¹ë³„ best-fit
        3) ê·¸ë˜ë„ ì—†ìœ¼ë©´ â‰¤ d ì „ì²´ì—ì„œ worst-fit ì¬ì•µì»¤
        4) ì „í˜€ ë¶ˆê°€ â†’ ('fallback', 0, old_slot)
        """
        d = int(self.deadline[v])
        old = int(self.lowest_version_slot[v])
        size0 = float(self.time_size[v, 0])
        total_size = self._combo_size(v, combo_idx)

        # 1) in-place: ì¦ë¶„ë§Œ í•„ìš”
        inc_same = total_size - size0
        if inc_same <= (self.time_limit[old] - self.time_used[old] + 1e-9):
            return ('inplace', combo_idx, old)

        # 2) ì°½ ë‚´ì—ì„œ 'ê°€ê¹Œìš´ ê·¸ë£¹ë¶€í„°' ê·¸ë£¹ë³„ best-fit
        lo = max(0, d - SLOT_CONCENTRATION)
        for g in range(d, lo - 1, -1):  # d, d-1, ..., lo
            cand = [
                i for i in range(self.n_slots)
                if (i % self.n_deadline) == g
                and i != old
                and total_size <= (self.time_limit[i] - self.time_used[i] + 1e-9)
            ]
            if cand:
                # best-fit (ë‚¨ëŠ” ìš©ëŸ‰ ìµœì†Œ). ë™ë¥ ì´ë©´ í˜„ì¬ ì‚¬ìš©ë¥ ì´ ë‚®ì€ ìŠ¬ë¡¯ì„ ì„ í˜¸(ë‹¨í¸í™”+ë°¸ëŸ°ìŠ¤)
                s = min(
                    cand,
                    key=lambda i: (
                        self._leftover_after(i, total_size),
                        self.time_used[i] / (self.time_limit[i] + 1e-9)
                    )
                )
                return ('reanchor_bestfit', combo_idx, s)

        # 3) â‰¤ d ì „ì²´ì—ì„œ worst-fit
        feasible = [
            i for i in range(self.n_slots)
            if (i % self.n_deadline) <= d
            and i != old
            and total_size <= (self.time_limit[i] - self.time_used[i] + 1e-9)
        ]
        if feasible:
            s = max(feasible, key=lambda i: self._leftover_after(i, total_size))
            return ('reanchor_worstfit', combo_idx, s)

        # 4) ì™„ì „ ë¶ˆê°€ â†’ ì½¤ë³´ 0 í´ë°±(ì•µì»¤ ìœ ì§€)
        return ('fallback', 0, old)

    def summarize_video_rank_noise(self, topk=20):
        t = self.true_popularity.sum(axis=1)
        p = self.pred_popularity.sum(axis=1)
        order_t = np.argsort(-t)
        order_p = np.argsort(-p)
        top_t = set(order_t[:topk]); top_p = set(order_p[:topk])
        jacc = len(top_t & top_p) / max(1, len(top_t | top_p))
        rank_t = np.empty_like(order_t); rank_t[order_t] = np.arange(1, len(order_t)+1)
        rank_p = np.empty_like(order_p); rank_p[order_p] = np.arange(1, len(order_p)+1)
        rho = float(np.corrcoef(rank_t, rank_p)[0, 1])
        print(f"[Video-Noise] J@{topk}={jacc:.3f}, Spearman Ï={rho:.3f}")

    def print_video_rank_two_columns(self, topn=20):
        true_video = self.true_popularity.sum(axis=1)
        pred_video = self.pred_popularity.sum(axis=1)
        order_true = np.argsort(-true_video)[:topn]
        order_pred = np.argsort(-pred_video)[:topn]
        print(f"\n[Video-level Ranking] Top-{topn}")
        print(f"{'TRUE rank (video ids):':24s}{order_true}")
        print(f"{'PRED rank (video ids):':24s}{order_pred}")

    def _util_stats(self, time_used: np.ndarray):
        util = time_used / (self.time_limit + 1e-9)
        avg = float(util.mean())
        mn = float(util.min())
        mx = float(util.max())
        zeros = np.where(util == 0.0)[0].tolist()
        return avg, mn, mx, zeros

    def _log_util_stats(self, time_used: np.ndarray, tag: str):
        avg, mn, mx, zeros = self._util_stats(time_used)
        print(f"ğŸ“Š[{tag}] ìŠ¬ë¡¯ ì´ìš©ë¥  - í‰ê· : {avg:.2f}, ìµœì†Œ: {mn:.2f}, ìµœëŒ€: {mx:.2f}")
        if zeros:
            print(f"ğŸ›‘[{tag}] ì´ìš©ë¥  0ì¸ ìŠ¬ë¡¯: {zeros}")
        else:
            print(f"âœ…[{tag}] ëª¨ë“  ìŠ¬ë¡¯ì´ ì¼ë¶€ë¼ë„ ì‚¬ìš©ë¨")

    def print_rank_two_columns(self, vid_ids=None):
        """
        ê° ë¹„ë””ì˜¤ì— ëŒ€í•´ (TRUE rank, PRED rank)ë¥¼ ë‘ ì¹¼ëŸ¼ìœ¼ë¡œ ì¶œë ¥.
        ì˜ˆ: [6 5 4 3 2 1 0] í˜•íƒœ (ë‚´ë¦¼ì°¨ìˆœ ë­í¬ ì¸ë±ìŠ¤)
        """
        true = self.true_popularity
        pred = self.pred_popularity
        n_videos, _ = true.shape

        if vid_ids is None:
            vid_ids = np.arange(min(5, n_videos))  # ê¸°ë³¸ 5ê°œ ìƒ˜í”Œ

        for vid in vid_ids:
            order_t = np.argsort(-true[vid])
            order_p = np.argsort(-pred[vid])
            print(f"\n[Video {vid}]")
            print(f"{'TRUE rank:':12s}{order_t}")
            print(f"{'PRED rank:':12s}{order_p}")

    def _print_video_rank_topk(self, k=20):
        t = self.true_popularity.sum(axis=1)
        p = self.pred_popularity.sum(axis=1)
        print("\n[Video-level Ranking] Top-{}" .format(k))
        print("TRUE:", np.argsort(-t)[:k])
        print("PRED:", np.argsort(-p)[:k])

    def summarize_video_noise_profile(self, head_pct=0.05, mid_range=(0.40, 0.60), tail_pct=0.05, show_deciles=True):
        """
        ë¹„ë””ì˜¤-ë ˆë²¨ p_true vs p_predì˜ ë³€í™”ê°€ head/mid/tail ì–´ë””ì— ë§ì´ ë“¤ì–´ê°”ëŠ”ì§€ ìš”ì•½.
        - |Î”|, ìƒëŒ€ë³€í™”, TVD, head/mid/tailì˜ ì§ˆëŸ‰ ë³€í™”ë¥¼ ì¶œë ¥
        - (hetero ê³„ì—´ì¼ ë•Œ) ê°€ì¤‘ì¹˜ wì™€ |Î”|ì˜ ìƒê´€ë„ ì¶œë ¥
        """
        eps = 1e-12
        pt = self.video_popularity_true.astype(np.float64)
        pp = self.video_popularity_pred.astype(np.float64)
        assert np.isclose(pt.sum(), 1.0, atol=1e-6), "pt sum!=1"
        assert np.isclose(pp.sum(), 1.0, atol=1e-6), "pp sum!=1"

        n = len(pt)
        order = np.argsort(-pt)              # true ì¸ê¸° ë‚´ë¦¼ì°¨ìˆœ(ë­í¬)
        delta = np.abs(pp - pt)
        rel = np.abs((pp - pt) / (pt + eps)) # ìƒëŒ€ë³€í™”

        # ë²„í‚· ì¸ë±ìŠ¤
        H = max(1, int(n * head_pct))
        T = max(1, int(n * tail_pct))
        M0 = int(n * mid_range[0])
        M1 = int(n * mid_range[1])
        idx_head = order[:H]
        idx_mid  = order[M0:M1]
        idx_tail = order[-T:]

        # í†µê³„ í•¨ìˆ˜
        def stats(idx):
            return dict(
                mean_abs_delta=float(delta[idx].mean()),
                mean_rel_delta=float(rel[idx].mean()),
                mass_true=float(pt[idx].sum()),
                mass_pred=float(pp[idx].sum())
            )

        s_head = stats(idx_head)
        s_mid  = stats(idx_mid)
        s_tail = stats(idx_tail)

        tv_total = 0.5 * float(np.abs(pp - pt).sum())
        tv_head  = 0.5 * float(np.abs(pp[idx_head] - pt[idx_head]).sum())
        tv_mid   = 0.5 * float(np.abs(pp[idx_mid]  - pt[idx_mid]).sum())
        tv_tail  = 0.5 * float(np.abs(pp[idx_tail] - pt[idx_tail]).sum())

        print("---- Video popularity noise profile ----")
        print(f"Total Variation Distance (all): {tv_total:.6f}")
        print(f"  â€¢ Head({head_pct*100:.1f}%): TV={tv_head:.6f} | |Î”|={s_head['mean_abs_delta']:.6e} | rel={s_head['mean_rel_delta']:.3f} | mass trueâ†’pred: {s_head['mass_true']:.3f}â†’{s_head['mass_pred']:.3f}")
        print(f"  â€¢ Mid ({int(mid_range[0]*100)}~{int(mid_range[1]*100)}%): TV={tv_mid:.6f} | |Î”|={s_mid['mean_abs_delta']:.6e} | rel={s_mid['mean_rel_delta']:.3f} | mass: {s_mid['mass_true']:.3f}â†’{s_mid['mass_pred']:.3f}")
        print(f"  â€¢ Tail({tail_pct*100:.1f}%): TV={tv_tail:.6f} | |Î”|={s_tail['mean_abs_delta']:.6e} | rel={s_tail['mean_rel_delta']:.3f} | mass: {s_tail['mass_true']:.3f}â†’{s_tail['mass_pred']:.3f}")

        # decileë³„ í‰ê·  |Î”| (ì„ íƒ)
        if show_deciles:
            dec = 10
            print("Decile mean |Î”| by true-rank (1=head â†’ 10=tail):")
            for d in range(dec):
                lo = int(n * d/dec); hi = int(n * (d+1)/dec)
                m = float(delta[order[lo:hi]].mean())
                print(f"  D{d+1}: {m:.6e}")

        # hetero ê³„ì—´ì´ë©´, ì„¤ê³„ ê°€ì¤‘ì¹˜ wì™€ |Î”|ì˜ ìƒê´€ í™•ì¸
        if "hetero" in str(self.video_noise_model):
            mode_map = {"hetero_gaussian": "mid",
                        "hetero_gaussian_head": "head",
                        "hetero_gaussian_tail": "tail"}
            mode = mode_map.get(self.video_noise_model, "mid")
            w = self._hetero_weight(pt, mode=mode, alpha=getattr(self, "hetero_alpha", 1.0))
            corr = np.corrcoef(w, delta)[0, 1]
            print(f"corr(|Î”|, weight[{mode}]) = {float(corr):.3f}")

    def summarize_video_head_mid_tail_jaccard(self, k_list=(10, 20, 50, 100)):
        pt = self.video_popularity_true
        pp = self.video_popularity_pred
        order_t = np.argsort(-pt)
        order_p = np.argsort(-pp)

        print("Top-K Jaccard (video-level, true vs pred):")
        for k in k_list:
            s_t = set(order_t[:k]); s_p = set(order_p[:k])
            j = len(s_t & s_p) / max(1, len(s_t | s_p))
            print(f"  K={k:4d}: J={j:.3f}")

        # Head/Mid/Tail ì§ˆëŸ‰(í™•ë¥ ) ì´ë™
        n = len(pt)
        head = int(0.05*n); tail = int(0.05*n)
        mid0, mid1 = int(0.40*n), int(0.60*n)
        H = order_t[:head]; M = order_t[mid0:mid1]; T = order_t[-tail:]
        def mass(idx, p): return float(p[idx].sum())
        print("Mass shift by rank buckets (trueâ†’pred):")
        print(f"  Head(Top5%): {mass(H, pt):.3f} â†’ {mass(H, pp):.3f}")
        print(f"  Mid (40~60%): {mass(M, pt):.3f} â†’ {mass(M, pp):.3f}")
        print(f"  Tail(Bot5%): {mass(T, pt):.3f} â†’ {mass(T, pp):.3f}")


    def _hetero_weight(self, p, mode='mid', alpha=1.0):
        import numpy as np
        eps = 1e-12
        p = np.asarray(p, dtype=np.float64)

        if mode == 'mid':
            w = np.sqrt(p * (1.0 - p))                   # ì¤‘ê°„ì—ì„œ ìµœëŒ€
        else:
            # ë­í¬ ê¸°ë°˜ ê°€ì¤‘: í° pê°€ ìƒìœ„(í—¤ë“œ)
            order = np.argsort(-p)                       # ë‚´ë¦¼ì°¨ìˆœ
            r = np.empty_like(order, dtype=np.float64)
            r[order] = np.linspace(0.0, 1.0, len(p))     # í—¤ë“œ~ê¼¬ë¦¬: 0â†’1
            if mode == 'head':
                w = (1.0 - r) ** alpha                   # í—¤ë“œ ìª½ ê°€ì¤‘â†‘
            elif mode == 'tail':
                w = (r) ** alpha                         # ê¼¬ë¦¬ ìª½ ê°€ì¤‘â†‘
            else:
                w = np.ones_like(p)

        w /= (w.max() + eps)                             # [0,1] ì •ê·œí™”
        return w
    
    def _perturb_video_prob_hetero_gaussian(self, p, sigma=0.05, hetero_beta=0.5,
                                        mode='mid', alpha=1.0, floor=1e-12, rng=None):
        import numpy as np
        if rng is None:
            rng = np.random.default_rng()
        p = np.asarray(p, dtype=np.float64)
        eps = 1e-12

        w = self._hetero_weight(p, mode=mode, alpha=alpha)
        sig = sigma * ((1.0 - hetero_beta) + hetero_beta * w)

        q = np.clip(p + rng.normal(0.0, sig, size=p.shape), 0, None)
        q = np.maximum(q, floor)
        q = q / (q.sum() + eps)
        return q.astype(np.float32)

 
    # ë¹„ë””ì˜¤ ì¸ê¸°ë„ ë³€ì´ í•¨ìˆ˜ë“¤ 3ê°œ 
    def _perturb_video_prob_gaussian(self, p, sigma=0.05, rng=None):
        if rng is None:
            rng = np.random.default_rng()
        q = p.astype(np.float64).copy()
        q = np.clip(q + rng.normal(0.0, sigma, size=q.shape), 0, None)
        s = q.sum()
        if s <= 0:
            q = np.ones_like(q) / len(q)
        else:
            q = q / s
        return q.astype(np.float32)
   

    # ë²„ì „ ì¸ê¸°ë„ ë³€ì´ í•¨ìˆ˜ 
    def _perturb_version_probs(self, base_probs, scale, kind, rng):

        probs = np.array(base_probs, dtype=np.float64)

        if scale > 0:
            if kind == "gaussian":
                noise = rng.normal(0, scale, size=len(probs))
                probs = np.clip(probs + noise, 0, None)

            elif kind == "dirichlet":
                alpha = np.clip(probs * (1.0 / scale), 1e-3, None)
                probs = rng.dirichlet(alpha)

            elif kind == "swap":
                probs = probs.copy()
                k = max(1, int(scale * len(probs)))
                idx = rng.choice(len(probs), size=2 * k, replace=False)
                for i in range(0, len(idx), 2):
                    probs[idx[i]], probs[idx[i+1]] = probs[idx[i+1]], probs[idx[i]]

        total = probs.sum()
        if total <= 0:
            # ëª¨ë“  ê°’ì´ 0ì´ë©´ ê· ë“±ë¶„í¬ë¡œ ì´ˆê¸°í™”
            probs = np.ones_like(probs) / len(probs)
        else:
            probs /= total

        return probs

    def init_popularity(self, skewness=None):
        """
        TRUE(oracle) ë¶„í¬ì™€ PRED(ì˜ˆì¸¡) ë¶„í¬ë¥¼ ë¹„ë””ì˜¤/ë²„ì „ ë ˆë²¨ì—ì„œ ë”°ë¡œ ë§Œë“¤ê³ ,
        ìµœì¢… pred_popularity = (ë¹„ë””ì˜¤ PRED) Ã— (ë²„ì „ PRED) ë¡œ êµ¬ì„±.
        """
        rng = np.random.default_rng()

        # ---------- (1) ë¹„ë””ì˜¤ ì¸ê¸°ë„ TRUE : Zipf ----------
        if skewness is None:
            skewness = np.random.uniform(0.5, 1.0)
            print("Zipf parameter:",skewness)

        ranks = np.arange(1, self.n_videos + 1)
        p_video_true = (1.0 / (ranks ** skewness))
        p_video_true = (p_video_true / p_video_true.sum()).astype(np.float32)
        self.video_popularity_true = p_video_true  # (n_videos,)

        

        # ---------- (1') ë¹„ë””ì˜¤ ì¸ê¸°ë„ PRED : noise_model ì„ íƒ ----------
        if  self.video_noise_model == "gaussian":
            self.video_popularity_pred = self._perturb_video_prob_gaussian(
                p_video_true,
                sigma=self.video_noise_param,
                rng=rng
            ).astype(np.float32)

        elif self.video_noise_model == "hetero_gaussian":
        # ê¸°ì¡´: ì¤‘ê°„ êµ¬ê°„ ê°•ì¡°
            self.video_popularity_pred = self._perturb_video_prob_hetero_gaussian(
                p_video_true, sigma=self.video_noise_param,
                hetero_beta=HETERO_BETA, mode='mid', alpha=getattr(self, "hetero_alpha", 1.0),
                floor=1e-6, rng=rng
            ).astype(np.float32)

        elif self.video_noise_model == "hetero_gaussian_head":
            self.video_popularity_pred = self._perturb_video_prob_hetero_gaussian(
                p_video_true, sigma=self.video_noise_param,
                hetero_beta=HETERO_BETA, mode='head', alpha=getattr(self, "hetero_alpha", 1.0),
                floor=1e-6, rng=rng
            ).astype(np.float32)

        elif self.video_noise_model == "hetero_gaussian_tail":
            self.video_popularity_pred = self._perturb_video_prob_hetero_gaussian(
                p_video_true, sigma=self.video_noise_param,
                hetero_beta=HETERO_BETA, mode='tail', alpha=getattr(self, "hetero_alpha", 1.0),
                floor=1e-6, rng=rng
            ).astype(np.float32)
        
        elif self.video_noise_model == "real_dataset_train":
            #rand_idx = random.randint(0, 39)
            #csv_path = rf"knn/{KNN}/knn_prediction_A_k{KNN}_{rand_idx}_.csv"
            #data = np.loadtxt(csv_path, delimiter=",", skiprows=1)
            #self.pred_rank = data[:,2]
            #self.video_popularity_pred = (1.0 / (self.pred_rank ** skewness))


            rand_idx = random.randint(0, 39)
            csv_path = rf"K241124/{KNN}/knn_prediction_A_k{KNN}_{rand_idx}_.csv"
            data = np.loadtxt(csv_path, delimiter=",", skiprows=1)
            self.video_popularity_true = data[:, 1]
            self.video_popularity_pred = data[:, 2]

            

        elif self.video_noise_model == "real_dataset_test":
            #rand_idx = random.randint(40, 49)
            #csv_path = rf"knn/{KNN}/knn_prediction_A_k{KNN}_{rand_idx}_.csv"
            #data = np.loadtxt(csv_path, delimiter=",", skiprows=1)
            #self.pred_rank = data[:,2]
            #self.video_popularity_pred = (1.0 / (self.pred_rank ** skewness))

            rand_idx = random.randint(40, 49)
            csv_path = rf"K241124/{KNN}/knn_prediction_A_k{KNN}_{rand_idx}_.csv"
            data = np.loadtxt(csv_path, delimiter=",", skiprows=1)
            self.video_popularity_true = data[:, 1]
            self.video_popularity_pred = data[:, 2]


        else:
            # ì§€ì • ì•ˆ í•˜ë©´ TRUE ê·¸ëŒ€ë¡œ


            #self.video_popularity_pred = self.video_popularity_true.copy()

            rand_idx = random.randint(0, 39)
            csv_path = rf"K241124/{KNN}/knn_prediction_A_k{KNN}_{rand_idx}_.csv"
            data = np.loadtxt(csv_path, delimiter=",", skiprows=1)
            self.video_popularity_true = data[:, 1]
            self.video_popularity_pred = data[:, 2]

        

        # ---------- (2) ë²„ì „ ì¸ê¸°ë„ TRUE : ì¤‘ì•™ ë²„ì „ ì¤‘ì‹¬ ê°€ìš°ì‹œì•ˆ ----------
        centers = np.arange(self.n_ver)
        self.version_popularity_true = np.zeros((self.n_videos, self.n_ver), dtype=np.float32)
        center = (self.n_ver - 1) / 2.0    # MVP
        #center = self.n_ver - 1             # HVP
        #center = 0.0                       # LVP
        std = self.n_ver / 4.0
        for vid in range(self.n_videos):
            probs = np.exp(-0.5 * ((centers - center) / std) ** 2)
            probs /= probs.sum()
            self.version_popularity_true[vid] = probs

        # ---------- (2') ë²„ì „ ì¸ê¸°ë„ PRED êµë€ ëª¨ë¸  ----------
        self.version_popularity_pred = np.zeros_like(self.version_popularity_true, dtype=np.float32)
        vm = self.version_noise_model
        for vid in range(self.n_videos):
            self.version_popularity_pred[vid] = self._perturb_version_probs(
                base_probs=self.version_popularity_true[vid],
                scale=self.version_noise_param,
                kind=vm,  # ë˜ëŠ” kind="dirichlet"/"gaussian"/"swap"ìœ¼ë¡œ ë§¤í•‘
                rng=rng
            )
        
        # ---------- (3) ê²°í•© ----------
        # TRUE ê²°í•©
        self.true_popularity = (self.version_popularity_true.T * self.video_popularity_true).T
        # PRED ê²°í•©
        pred_from_ver = (self.version_popularity_pred.T * self.video_popularity_pred).T
        self.pred_popularity = pred_from_ver

        # ---------- (4) í˜¸í™˜ì„± ìœ ì§€ ----------
        # ê¸°ì¡´ ì½”ë“œê°€ self.popularity ë¥¼ ì°¸ì¡°í•˜ë”ë¼ë„ TRUEì™€ ë™ì¼í•˜ë„ë¡ ë‘¡ë‹ˆë‹¤.
        self.popularity = self.true_popularity

        # ëª¨ë‘ ì•ˆë‹¤ë¼ê³  ê°€ì •í• ë•Œ
        # self.pred_popularity = self.popularity
        

    

    def reset(self, seed=None, options=None):
        # ----- RNG -----
        if seed is not None:
            np.random.seed(seed)
        if TRAINING:
            # ë©€í‹°í”„ë¡œì„¸ìŠ¤ í•™ìŠµ ì‹œ ì—í”¼ì†Œë“œ/í”„ë¡œì„¸ìŠ¤ë³„ ì‹œë“œ ì°¨ë³„í™”
            np.random.seed(self.episode_count + os.getpid())
        if TESTING: 
            np.random.seed(self.episode_count + os.getpid())
        

        self.allocation_dict = {s: [] for s in range(self.n_slots)}

        '''
        self.vmaf = np.zeros((self.n_videos, self.n_ver), dtype=np.float32)
        vmaf_means = np.linspace(40, 100, self.n_ver)
        vmaf_std = 5.0

        for vid in range(self.n_videos):
            vmaf_values = np.clip(np.random.normal(loc=vmaf_means, scale=vmaf_std), 0, 100)
            vmaf_values = np.sort(vmaf_values)
            vmaf_values[-1] = 100.0
            self.vmaf[vid, :] = vmaf_values

        self.video_length = np.full(self.n_videos, 300, dtype=np.float32)
        base_time_size = (self.bitrates[None, :] * self.video_length[:, None]) / (8 * 1024)

        # Time Distribution
        size_noise_std = 0.000001
        rng = np.random.default_rng()
        mult = rng.lognormal(mean=0.0, sigma=size_noise_std, size=base_time_size.shape)
        self.time_size = (base_time_size * mult).astype(np.float32)
        self.time_size=base_time_size
        '''

        self.reset_size_factor = SIZE_FACTOR

        if TRAINING:
            self.reset_size_factor = np.random.uniform(0.2, 0.5)
            #self.reset_size_factor = SIZE_FACTOR
            total_time_size = np.sum(self.time_size)
            total_time_budget = total_time_size * self.reset_size_factor
            self.time_limit = np.full(self.n_slots, total_time_budget / self.n_slots, dtype=np.float32)  # (n_slots,)
        

        self._resample_deadlines() 

        # ----- ì´ˆê¸°í™” -----
        self.current_video = 0
        self.time_used = np.zeros(self.n_slots, dtype=np.float32)
        self.total_reward = 0.0
        # (ì„ íƒ) ê¸°ì¡´ ê¸°ë¡ ì´ˆê¸°í™”
        #self.allocation_dict = {s: [] for s in range(self.n_slots)}

        # ----- ver=0 ì„ í• ë‹¹: ë°ë“œë¼ì¸-í˜¸í™˜ ìŠ¬ë¡¯ë§Œ + ë² ìŠ¤íŠ¸í• -----
        # Strict: slot_group == deadline[v]
        # Fallback: (í•„ìš” ì‹œ) slot_group <= deadline[v]
        self.lowest_version_slot = np.zeros(self.n_videos, dtype=int)
        for v in range(self.n_videos):
            size0 = float(self.time_size[v, 0])
            d = int(self.deadline[v])

            # 1) strict í›„ë³´
            strict_candidates = [
                i for i in range(self.n_slots)
                if (i % self.n_deadline) == d and (self.time_used[i] + size0) <= self.time_limit[i]
            ]

            # 2) strictê°€ ê½‰ ì°¼ìœ¼ë©´ ì™„í™”(â‰¤ d)
            if not strict_candidates:
                le_candidates = [
                    i for i in range(self.n_slots)
                    if (i % self.n_deadline) <= d and (self.time_used[i] + size0) <= self.time_limit[i]
                ]
            else:
                le_candidates = []

            candidates = strict_candidates if strict_candidates else le_candidates

            if candidates:
                # ë² ìŠ¤íŠ¸í•: ë‚¨ëŠ” ìš©ëŸ‰ ìµœì†Œ ìŠ¬ë¡¯ ì„ íƒ â†’ ë‹¨í¸í™” ê°ì†Œ
                def leftover(i):
                    return float(self.time_limit[i] - (self.time_used[i] + size0))
                idx = min(candidates, key=leftover)

                self.lowest_version_slot[v] = idx
                self.time_used[idx] += size0
                # (ì˜µì…˜) ê¸°ë¡
                # self.allocation_dict[idx].append((v, 0))
            else:
                # ëª¨ë“  í›„ë³´ê°€ ë¶ˆê°€(ë“œë¬¾): ê°€ì¥ ì—¬ìœ ë¡œìš´ ìŠ¬ë¡¯ì— ì‹œë„(ì˜¤ë²„ ë°©ì§€)
                idx = int(np.argmin(self.time_used / (self.time_limit + 1e-6)))
                if (self.time_used[idx] + size0) <= self.time_limit[idx]:
                    self.lowest_version_slot[v] = idx
                    self.time_used[idx] += size0
                    # self.allocation_dict[idx].append((v, 0))
                # else: ver=0ì¡°ì°¨ ëª» ë„£ëŠ” ê·¹íˆ ì˜ˆì™¸ ìƒí™© â†’ ì´í›„ ë‹¨ê³„ì—ì„œ ìì—°ìŠ¤ë ˆ fallback ì²˜ë¦¬

        # ----- ì¸ê¸°ë„ ì´ˆê¸°í™” -----  Variable Zipf
        if FIXED_ZIPF==1:
            skew_parameter = ZIPF_PARAMETER # í•„ìš” ì‹œ ì¡°ì •
            self.init_popularity(skew_parameter) 
        else:
            self.init_popularity(None)
        
        self.base_qoe_true = (self.true_popularity.sum(axis=1) * self.vmaf[:, 0]).astype(np.float64)
  
        # ----- ê¸°ëŒ€ QoE(ìŠ¤ì¼€ì¼ë§ ê¸°ì¤€) ê³„ì‚° -----
        self.expected_total_reward = 0.0
        for v in range(self.n_videos):
            self.expected_total_reward += float(np.sum(self.true_popularity[v] * self.vmaf[v]))
        
        # Noise Statistics 
        """
        if TESTING and self.episode_count == 0:
            self.summarize_video_noise_profile()
            self.summarize_video_head_mid_tail_jaccard()
        """

        # ----- ì´ˆê¸° ìƒíƒœ/ë§ˆìŠ¤í¬ ë°˜í™˜ -----
        self.mask_done = False
        self.state = self._compute_state()
        info = {'action_mask': self.get_valid_action_mask()}

        return self.state.astype(np.float32), info


    def get_valid_action_mask(self):
        v = int(self.current_video)
        mask = np.zeros(self.n_combos, dtype=bool)

        # ê° ì½¤ë³´ê°€ í´ë°± ì—†ì´ ë°°ì¹˜ ê°€ëŠ¥í•œì§€ ê²€ì‚¬
        for c in range(self.n_combos):
            plan = self._pick_slot_for_combo(v, c)
            if plan[0] != 'fallback':
                mask[c] = True

        # ì•ˆì „ì¥ì¹˜: combo=0 ì€ í•­ìƒ í—ˆìš©
        mask[0] = True
        return mask

    # Average utilization for each deadline slot 
    def _avg_util_by_deadline(self):
        util = self.time_used / (self.time_limit + 1e-9)  # [0,1] ê·¼ì²˜
        out = np.zeros(self.n_deadline, dtype=np.float32)
        for d in range(self.n_deadline):
            mask = (self.slot_groups == d)
            out[d] = util[mask].mean() if np.any(mask) else 0.0
        return np.clip(out, 0.0, 1.0)

    # Observation Space
    def _compute_state(self):
        vid = int(self.current_video)

        combo_qoe = self._combo_fallback_qoe_vec(vid, use="pred").astype(np.float32)  # (64,)
        total_norm, delta_norm = self._combo_size_vecs(vid)                           # (64,), (64,)

        avg_util_global = np.mean(self.time_used / (self.time_limit + 1e-6)).astype(np.float32)
        avg_util_global = np.array([avg_util_global], dtype=np.float32)

        denom = max(1, self.n_videos - 1)
        progress = np.array([self.current_video / denom], dtype=np.float32)

        avg_util_by_deadline = self._avg_util_by_deadline()  # (n_deadline,)
        norm_deadline = np.array([ self.deadline[vid] / (self.n_deadline - 1) ], dtype=np.float32)  # (1,)

        avg_util_global = np.clip(avg_util_global, 0.0, 1.0)
        progress = np.clip(progress, 0.0, 1.0)
        norm_deadline = np.clip(norm_deadline, 0.0, 1.0)

        size_factor = np.array([self.reset_size_factor], dtype=np.float32)


        state = np.concatenate([
            combo_qoe, total_norm, delta_norm, 
            avg_util_global, progress,
            avg_util_by_deadline, norm_deadline, size_factor
        ]).astype(np.float32)

        return state




    def step(self, action):
        v = int(self.current_video)
        combo = int(action)               
        plan_type, used_combo, slot = self._pick_slot_for_combo(v, combo)
        versions = self._versions_of_combo(used_combo)

        old_slot = int(self.lowest_version_slot[v])
        size0 = float(self.time_size[v, 0])
        total_size = float(np.sum(self.time_size[v, versions]))

        # ë°°ì¹˜ ì ìš©
        if slot == old_slot:
            # in-place ë˜ëŠ” fallback(ver0 ìœ ì§€)
            add_size = total_size - size0   # fallback(used_combo=0)ë©´ 0
            if add_size > 0:
                self.time_used[slot] += add_size
        else:
            # ì¬ì•µì»¤: oldì—ì„œ ver0 ì œê±° í›„ ìƒˆ ìŠ¬ë¡¯ì— ì „ì²´ ì¶”ê°€
            self.time_used[old_slot] -= size0
            self.time_used[old_slot] = max(0.0, self.time_used[old_slot])
            self.time_used[slot] += total_size
            self.lowest_version_slot[v] = slot

        # ë°°ì¹˜ ê¸°ë¡
        self.allocation_dict[slot].append((v, versions))
        


        # ë¦¬ì›Œë“œ(í´ë°± ë°˜ì˜)
        fb = []
        for ver in range(self.n_ver):
            if ver in versions:
                fb.append(ver)
            else:
                lower = [x for x in versions if x < ver]
                fb.append(max(lower) if lower else 0)

        # ê¸°ì¡´ fb ê³„ì‚° ê·¸ëŒ€ë¡œ
        #new_qoe = float(np.sum(self.pred_popularity[v] * self.vmaf[v, fb]))
        new_qoe = float(np.sum(self.true_popularity[v] * self.vmaf[v, fb]))
        delta_qoe = new_qoe - float(self.base_qoe_true[v])          # â† ì¸ê¸°ë„ ê°€ì¤‘ Î”

        # ì „ì—­ ì •ê·œí™”ë¡œ ìŠ¤ì¼€ì¼ë§Œ ì•ˆì •í™”(ì¸ê¸°ë„ ì°¨ì´ëŠ” ìœ ì§€)
        reward = (delta_qoe / (self.expected_total_reward + 1e-6)) * 1000.0

        # ì§„í–‰
        self.total_reward += reward
        self.current_video += 1

        done = (self.current_video >= self.n_videos)   # ë§ˆì§€ë§‰ê¹Œì§€ ì²˜ë¦¬í•˜ë©´ ì¢…ë£Œ
        truncated = False

        if not done:
            self.state = self._compute_state()
            info = {'action_mask': self.get_valid_action_mask()}
        else:
            info = {
                'terminal_observation': self.state.copy(),
                'episode': {'r': float(self.total_reward), 'l': int(self.n_videos)}
            }
            self.episode_count += 1
            #print(f"ğŸ¬ ì—í”¼ì†Œë“œ {self.episode_count} ì¢…ë£Œ - ì´ ë³´ìƒ: {self.total_reward:.2f}")
            
            
            
            if TESTING:
                
                self.HUF_value, self.HUF_alloc               = self.greedy_allocation_HUF()
                self.HUTF_value, self.HUTF_alloc             = self.greedy_allocation_HUTF()
                self.HUF_strict_value, self.HUF_strict_alloc = self.greedy_allocation_HUF_strictDL()
                self.HUTF_strict_value, self.HUTF_strict_alloc = self.greedy_allocation_HUTF_strictDL()
                self.MCKP_predict_value, self.MCKP_predict_alloc = self.greedy_allocation_combo_delta(mode="mckp")
                self.MCKP_true_value, self.MCKP_true_alloc      = self.greedy_allocation_combo_delta(mode="oracle")
                

                '''
                self.HUF_value = self.greedy_allocation_HUF()
                self.HUTF_value = self.greedy_allocation_HUTF()
                self.HUF_strict_value = self.greedy_allocation_HUF_strictDL()
                self.HUTF_strict_value = self.greedy_allocation_HUTF_strictDL()
                self.MCKP_predict_value = self.greedy_allocation_combo_delta(mode="mckp")     # ì˜ˆì¸¡ ë¶„í¬ ê¸°ë°˜ (MCKP)
                self.MCKP_true_value = self.greedy_allocation_combo_delta(mode="oracle")   # ì‹¤ì œ ë¶„í¬ ê¸°ë°˜ (Oracle)
                '''
                actual_qoe, full_qoe = self.compute_final_qoe(self.allocation_dict)
                rl_scaled10 = float((actual_qoe / (full_qoe + 1e-6)) * 10.0)  # â† ë² ì´ìŠ¤ë¼ì¸(ê·¸ë¦¬ë””)ì™€ ë™ì¼ ìŠ¤ì¼€ì¼
                #print("DRL:",rl_scaled10*100)
                self.PPO_value = rl_scaled10*100
                #print("time used : ", np.sum(self.time_used))
                #exit()


            # âœ… ìŠ¬ë¡¯ ì´ìš©ë¥  ìš”ì•½ ì¶œë ¥
            slot_utilization = self.time_used / self.time_limit
            avg_util = np.mean(slot_utilization)
            min_util = np.min(slot_utilization)
            max_util = np.max(slot_utilization)

            zero_util_slots = np.where(slot_utilization == 0.0)[0]  # ì´ìš©ë¥ ì´ 0ì¸ ìŠ¬ë¡¯ ì¸ë±ìŠ¤ ì¶”ì¶œ

            #print(f"ğŸ“Š ìŠ¬ë¡¯ ì´ìš©ë¥  ìš”ì•½ - í‰ê· : {avg_util:.2f}, ìµœì†Œ: {min_util:.2f}, ìµœëŒ€: {max_util:.2f}")

            if len(zero_util_slots) > 0:
                aaa=1
                #print(f"ğŸ›‘ ì´ìš©ë¥  0ì¸ ìŠ¬ë¡¯: {zero_util_slots.tolist()}")
  
        return self.state.astype(np.float32), reward, done, truncated, info

    def greedy_allocation_HUF(self):
        time_used = np.zeros(self.n_slots, dtype=np.float32)
        allocation_dict = {i: [] for i in range(self.n_slots)}
        assigned = np.zeros((self.n_videos, self.n_ver), dtype=bool)

        # (0-1) lowest version ì„ í• ë‹¹ (ver=0), load balancing + deadline ê³ ë ¤
        lowest_slot = {}
        for v in range(self.n_videos):
            size = self.time_size[v, 0]
            candidate_slots = [
                i for i in range(self.n_slots)
                if self.deadline[v] >= (i % self.n_deadline) and time_used[i] + size <= self.time_limit[i]
            ]
            if candidate_slots:
                idx = min(candidate_slots, key=lambda i: time_used[i] / (self.time_limit[i] + 1e-6))
                time_used[idx] += size
                allocation_dict[idx].append((v, 0))
                assigned[v, 0] = True
                lowest_slot[v] = idx

        # (1) score ê³„ì‚° ë° ì •ë ¬
        score_list = []
        for v in range(self.n_videos):
            for ver in range(1, self.n_ver):  # lowest ì œì™¸
                #score = self.popularity[v, ver] * self.vmaf[v, ver]
                score = self.pred_popularity[v, ver] * self.vmaf[v, ver]
                score_list.append((score, v, ver))
        score_list.sort(reverse=True)

        # (2) ë¹„ë””ì˜¤ë³„ ìµœì´ˆ ë“±ì¥ verê³¼ ì´í›„ ì²˜ë¦¬
        slot_per_video = {}  # v: slot index

        for _, v, ver in score_list:
            if assigned[v, ver]:
                continue
            size = self.time_size[v, ver]

            if v not in slot_per_video:
            # ìµœì´ˆ ë“±ì¥í•œ ë¹„ë””ì˜¤ì¸ ê²½ìš°: lowest + í˜„ì¬ ver ì¬í• ë‹¹ ì‹œë„
                lowest_size = self.time_size[v, 0]


                candidates = [
                    i for i in range(self.n_slots)
                    if self.deadline[v] >= (i % self.n_deadline) and \
                    time_used[i] + lowest_size + size <= self.time_limit[i]
                ]

                if candidates:
                    # í›„ë³´ ì¤‘ í˜„ì¬ ì±„ì›€ ë¹„ìœ¨ì´ ê°€ì¥ ë‚®ì€ ìŠ¬ë¡¯ ì„ íƒ
                    idx = min(candidates, key=lambda i: time_used[i] / (self.time_limit[i] + 1e-6))

                    if v in lowest_slot:
                        prev = lowest_slot[v]
                        time_used[prev] -= self.time_size[v, 0]
                        allocation_dict[prev] = [item for item in allocation_dict[prev] if not (item[0] == v and item[1] == 0)]
                        assigned[v, 0] = False

                    time_used[idx] += lowest_size + size
                    allocation_dict[idx].append((v, 0))
                    allocation_dict[idx].append((v, ver))
                    assigned[v, 0] = True
                    assigned[v, ver] = True
                    slot_per_video[v] = idx



                '''
                for i in range(self.n_slots):
                    if self.deadline[v] >= (i % self.n_deadline) and \
                    time_used[i] + lowest_size + size <= self.time_limit[i]:
                    # ê¸°ì¡´ lowest í• ë‹¹ ì·¨ì†Œ
                        if v in lowest_slot:
                            prev = lowest_slot[v]
                            time_used[prev] -= self.time_size[v, 0]
                            allocation_dict[prev] = [
                                item for item in allocation_dict[prev] if not (item[0] == v and item[1] == 0)
                            ]
                            assigned[v, 0] = False

                    # ì¬í• ë‹¹
                        time_used[i] += lowest_size + size
                        allocation_dict[i].append((v, 0))
                        allocation_dict[i].append((v, ver))
                        assigned[v, 0] = True
                        assigned[v, ver] = True
                        slot_per_video[v] = i
                        break
                '''
            else:
            # ì´ë¯¸ ë“±ì¥í•œ ë¹„ë””ì˜¤ì˜ ê²½ìš° í•´ë‹¹ ìŠ¬ë¡¯ì—ë§Œ í• ë‹¹
                i = slot_per_video[v]
                if time_used[i] + size <= self.time_limit[i]:
                    time_used[i] += size
                    allocation_dict[i].append((v, ver))
                    assigned[v, ver] = True

        # (3) reward ê³„ì‚° (fallback í¬í•¨)
        total_reward = 0.0
        for v in range(self.n_videos):
            allocated_versions = [ver for i in range(self.n_slots)
                                  for (vid, ver) in allocation_dict[i] if vid == v]
            fallback_versions = []
            for ver in range(self.n_ver):
                if ver in allocated_versions:
                    fallback_versions.append(ver)
                else:
                    lower = [v2 for v2 in allocated_versions if v2 < ver]
                    fallback_versions.append(max(lower) if lower else 0)

            #version_popularities = self.popularity[v]
            version_popularities = self.true_popularity[v]
            version_vmaf = self.vmaf[v, fallback_versions]
            reward = np.sum(version_popularities * version_vmaf) * 100
            total_reward += reward

        # (4) ìŠ¤ì¼€ì¼ë§
        scaled_reward = (total_reward / (self.expected_total_reward + 1e-6)) * 10.0
        #print(f"ğŸ¯ ê·¸ë¦¬ë””(HUF) ì´ QoE (ìŠ¤ì¼€ì¼ëœ): {scaled_reward:.2f}")
        # self._log_util_stats(time_used, "Greedy HUF")
        
        return scaled_reward, allocation_dict
    

    def greedy_allocation_HUTF(self):
        time_used = np.zeros(self.n_slots, dtype=np.float32)
        allocation_dict = {i: [] for i in range(self.n_slots)}
        assigned = np.zeros((self.n_videos, self.n_ver), dtype=bool)

        # (0-1) lowest version ì„ í• ë‹¹ (ver=0), load balancing + deadline ê³ ë ¤
        lowest_slot = {}
        for v in range(self.n_videos):
            size = self.time_size[v, 0]
            candidate_slots = [
                i for i in range(self.n_slots)
                if self.deadline[v] >= (i % self.n_deadline) and time_used[i] + size <= self.time_limit[i]
            ]
            if candidate_slots:
                idx = min(candidate_slots, key=lambda i: time_used[i] / (self.time_limit[i] + 1e-6))
                time_used[idx] += size
                allocation_dict[idx].append((v, 0))
                assigned[v, 0] = True
                lowest_slot[v] = idx

        # (1) score ê³„ì‚° ë° ì •ë ¬
        score_list = []
        for v in range(self.n_videos):
            for ver in range(1, self.n_ver):  # lowest ì œì™¸
                #score = (self.popularity[v, ver] * self.vmaf[v, ver]) / (self.time_size[v, ver] + 1e-6)
                score = (self.pred_popularity[v, ver] * self.vmaf[v, ver]) / (self.time_size[v, ver] + 1e-6)
                score_list.append((score, v, ver))
        score_list.sort(reverse=True)

        # (2) ë¹„ë””ì˜¤ë³„ ìµœì´ˆ ë“±ì¥ verê³¼ ì´í›„ ì²˜ë¦¬
        slot_per_video = {}  # v: slot index

        for _, v, ver in score_list:
            if assigned[v, ver]:
                continue
            size = self.time_size[v, ver]

            if v not in slot_per_video:
            # ìµœì´ˆ ë“±ì¥í•œ ë¹„ë””ì˜¤ì¸ ê²½ìš°: lowest + í˜„ì¬ ver ì¬í• ë‹¹ ì‹œë„
                lowest_size = self.time_size[v, 0]


                candidates = [
                    i for i in range(self.n_slots)
                    if self.deadline[v] >= (i % self.n_deadline) and \
                    time_used[i] + lowest_size + size <= self.time_limit[i]
                ]

                if candidates:
                    # í›„ë³´ ì¤‘ í˜„ì¬ ì±„ì›€ ë¹„ìœ¨ì´ ê°€ì¥ ë‚®ì€ ìŠ¬ë¡¯ ì„ íƒ
                    idx = min(candidates, key=lambda i: time_used[i] / (self.time_limit[i] + 1e-6))

                    if v in lowest_slot:
                        prev = lowest_slot[v]
                        time_used[prev] -= self.time_size[v, 0]
                        allocation_dict[prev] = [item for item in allocation_dict[prev] if not (item[0] == v and item[1] == 0)]
                        assigned[v, 0] = False

                    time_used[idx] += lowest_size + size
                    allocation_dict[idx].append((v, 0))
                    allocation_dict[idx].append((v, ver))
                    assigned[v, 0] = True
                    assigned[v, ver] = True
                    slot_per_video[v] = idx




                '''
                for i in range(self.n_slots):
                    if self.deadline[v] >= (i % self.n_deadline) and \
                    time_used[i] + lowest_size + size <= self.time_limit[i]:
                    # ê¸°ì¡´ lowest í• ë‹¹ ì·¨ì†Œ
                        if v in lowest_slot:
                            prev = lowest_slot[v]
                            time_used[prev] -= self.time_size[v, 0]
                            allocation_dict[prev] = [
                                item for item in allocation_dict[prev] if not (item[0] == v and item[1] == 0)
                            ]
                            assigned[v, 0] = False

                    # ì¬í• ë‹¹
                        time_used[i] += lowest_size + size
                        allocation_dict[i].append((v, 0))
                        allocation_dict[i].append((v, ver))
                        assigned[v, 0] = True
                        assigned[v, ver] = True
                        slot_per_video[v] = i
                        break
                '''
            else:
            # ì´ë¯¸ ë“±ì¥í•œ ë¹„ë””ì˜¤ì˜ ê²½ìš° í•´ë‹¹ ìŠ¬ë¡¯ì—ë§Œ í• ë‹¹
                i = slot_per_video[v]
                if time_used[i] + size <= self.time_limit[i]:
                    time_used[i] += size
                    allocation_dict[i].append((v, ver))
                    assigned[v, ver] = True

        # (3) reward ê³„ì‚° (fallback í¬í•¨)
        total_reward = 0.0
        for v in range(self.n_videos):
            allocated_versions = [ver for i in range(self.n_slots)
                                  for (vid, ver) in allocation_dict[i] if vid == v]
            fallback_versions = []
            for ver in range(self.n_ver):
                if ver in allocated_versions:
                    fallback_versions.append(ver)
                else:
                    lower = [v2 for v2 in allocated_versions if v2 < ver]
                    fallback_versions.append(max(lower) if lower else 0)

            #version_popularities = self.popularity[v]
            version_popularities = self.true_popularity[v]
            version_vmaf = self.vmaf[v, fallback_versions]
            reward = np.sum(version_popularities * version_vmaf) * 100
            total_reward += reward

        # (4) ìŠ¤ì¼€ì¼ë§
        scaled_reward = (total_reward / (self.expected_total_reward + 1e-6)) * 10.0
        #print(f"ğŸ¯ ê·¸ë¦¬ë””(HUTF) ì´ QoE (ìŠ¤ì¼€ì¼ëœ): {scaled_reward:.2f}")
        # self._log_util_stats(time_used, "Greedy HUTF")
        
        return scaled_reward, allocation_dict

    def greedy_allocation_HUF_strictDL(self):
        time_used = np.zeros(self.n_slots, dtype=np.float32)
        allocation_dict = {i: [] for i in range(self.n_slots)}
        assigned = np.zeros((self.n_videos, self.n_ver), dtype=bool)

        # (0-1) lowest version ì„ í• ë‹¹ (ver=0), load balancing + strict deadline
        lowest_slot = {}
        for v in range(self.n_videos):
            size = self.time_size[v, 0]
            candidate_slots = [
                i for i in range(self.n_slots)
                if self.deadline[v] == (i % self.n_deadline) and time_used[i] + size <= self.time_limit[i]
            ]
            if candidate_slots:
                idx = min(candidate_slots, key=lambda i: time_used[i] / (self.time_limit[i] + 1e-6))
                time_used[idx] += size
                allocation_dict[idx].append((v, 0))
                assigned[v, 0] = True
                lowest_slot[v] = idx

        # (1) score ê³„ì‚° ë° ì •ë ¬
        score_list = []
        for v in range(self.n_videos):
            for ver in range(1, self.n_ver):
                #score = self.popularity[v, ver] * self.vmaf[v, ver]
                score = self.pred_popularity[v, ver] * self.vmaf[v, ver]
                score_list.append((score, v, ver))
        score_list.sort(reverse=True)

        # (2) ë¹„ë””ì˜¤ë³„ ìµœì´ˆ ë“±ì¥ verê³¼ ì´í›„ ì²˜ë¦¬
        slot_per_video = {}

        for _, v, ver in score_list:
            if assigned[v, ver]:
                continue
            size = self.time_size[v, ver]

            if v not in slot_per_video:
                lowest_size = self.time_size[v, 0]

                candidates = [
                    i for i in range(self.n_slots)
                    if self.deadline[v] == (i % self.n_deadline) and \
                    time_used[i] + lowest_size + size <= self.time_limit[i]
                ]

                if candidates:
                    # í›„ë³´ ì¤‘ í˜„ì¬ ì±„ì›€ ë¹„ìœ¨ì´ ê°€ì¥ ë‚®ì€ ìŠ¬ë¡¯ ì„ íƒ
                    idx = min(candidates, key=lambda i: time_used[i] / (self.time_limit[i] + 1e-6))

                    if v in lowest_slot:
                        prev = lowest_slot[v]
                        time_used[prev] -= self.time_size[v, 0]
                        allocation_dict[prev] = [item for item in allocation_dict[prev] if not (item[0] == v and item[1] == 0)]
                        assigned[v, 0] = False

                    time_used[idx] += lowest_size + size
                    allocation_dict[idx].append((v, 0))
                    allocation_dict[idx].append((v, ver))
                    assigned[v, 0] = True
                    assigned[v, ver] = True
                    slot_per_video[v] = idx



                '''
                for i in range(self.n_slots):
                    if self.deadline[v] == (i % self.n_deadline) and \
                    time_used[i] + lowest_size + size <= self.time_limit[i]:
                        if v in lowest_slot:
                            prev = lowest_slot[v]
                            time_used[prev] -= self.time_size[v, 0]
                            allocation_dict[prev] = [item for item in allocation_dict[prev] if not (item[0] == v and item[1] == 0)]
                            assigned[v, 0] = False
                        time_used[i] += lowest_size + size
                        allocation_dict[i].append((v, 0))
                        allocation_dict[i].append((v, ver))
                        assigned[v, 0] = True
                        assigned[v, ver] = True
                        slot_per_video[v] = i
                        break
                '''
            else:
                i = slot_per_video[v]
                if time_used[i] + size <= self.time_limit[i]:
                    time_used[i] += size
                    allocation_dict[i].append((v, ver))
                    assigned[v, ver] = True

        total_reward = 0.0
        for v in range(self.n_videos):
            allocated_versions = [ver for i in range(self.n_slots)
                                  for (vid, ver) in allocation_dict[i] if vid == v]
            fallback_versions = []
            for ver in range(self.n_ver):
                if ver in allocated_versions:
                    fallback_versions.append(ver)
                else:
                    lower = [v2 for v2 in allocated_versions if v2 < ver]
                    fallback_versions.append(max(lower) if lower else 0)

            #version_popularities = self.popularity[v]
            version_popularities = self.true_popularity[v]
            version_vmaf = self.vmaf[v, fallback_versions]
            reward = np.sum(version_popularities * version_vmaf) * 100
            total_reward += reward

        scaled_reward = (total_reward / (self.expected_total_reward + 1e-6)) * 10.0
        
        #print(f"âœ¨ ê·¸ë¦¬ë””(HUF-StrictDL) ì´ QoE (ìŠ¤ì¼€ì¼ëœ): {scaled_reward:.2f}")

        return scaled_reward, allocation_dict
    
    def greedy_allocation_combo_delta(self, mode="mckp", *, verbose=True):
        """
        í†µí•© Î”-ê·¸ë¦¬ë”” (ì¡°í•© ë‹¨ìœ„, í•­ìƒ ver0 í¬í•¨)
        - mode="mckp": ì •ë ¬/ì„ íƒ(ë­í‚¹)ì— pred_popularity ì‚¬ìš© (ì´ì „ greedy_allocation_MCKP)
        - mode="oracle": ì •ë ¬/ì„ íƒ(ë­í‚¹)ì— true_popularity ì‚¬ìš© (ì´ì „ greedy_allocation_Oracle)
        ê³µí†µ ì •ì±…:
        * ver0 ì„ í• ë‹¹ (STRICT deadline ê·¸ë£¹ ë‚´ì—ì„œ load balancing)
        * ë¹„ë””ì˜¤ë³„ ì¡°í•© ìƒì„± â†’ Î”QoE/Î”Size â†’ (Î”QoE>0, Î”Size>0)ë§Œ ë‚¨ê¸°ê³  íŒŒë ˆí†  ì „ì„ 
        * ì „ì—­ score = Î”QoE_rank / (Î”Size+eps) ë‚´ë¦¼ì°¨ìˆœ
        * in-place ì—…ê·¸ë ˆì´ë“œ ìš°ì„ , ë¶ˆê°€ ì‹œ strict-deadline ë‚´ worst-fit ì¬ì•µì»¤
        * ìµœì¢… í‰ê°€ëŠ” í•­ìƒ true_popularity Ã— VMAF (fallback)ë¡œ ê³„ì‚°
        """
        import math
        import numpy as np

        assert mode in ("mckp", "oracle")
        eps = 1e-9

        # -------------------------- í—¬í¼ --------------------------
        def _qoe_for_combo(v, versions_set, use="true"):
            """
            ë¹„ë””ì˜¤ vê°€ versions_set(í•­ìƒ 0 í¬í•¨)ë§Œ ì €ì¥ëì„ ë•Œì˜ QoE.
            use: "true" -> true_popularity, "pred" -> pred_popularity
            """
            dist = self.true_popularity if use == "true" else self.pred_popularity
            used = sorted(versions_set)
            # fallback: ìš”ì²­ verì— ëŒ€í•´ ì €ì¥ëœ <= ver ì¤‘ ìµœëŒ€, ì—†ìœ¼ë©´ 0
            fb = []
            for ver in range(self.n_ver):
                if ver in used:
                    fb.append(ver)
                else:
                    lowers = [x for x in used if x < ver]
                    fb.append(max(lowers) if lowers else 0)
            fb = np.array(fb, dtype=int)
            return float(np.sum(dist[v] * self.vmaf[v, fb]))

        def _size_for_combo(v, versions_set):
            """ë¹„ë””ì˜¤ vê°€ versions_set ì €ì¥ ì‹œ ì´ ìš©ëŸ‰(ì‹œê°„)"""
            idx = list(versions_set)
            return float(np.sum(self.time_size[v, idx]))

        def _pareto_prune(cands):
            """
            cands: list of (delta_qoe, delta_size, mask_set)
            í‡´í–‰ ì œê±°(Î”QoE<=0 & Î”Size>0) + íŒŒë ˆí†  ì „ì„ ë§Œ ë‚¨ê¹€.
            """
            filt = [(dq, ds, ms) for (dq, ds, ms) in cands if (dq > 0 and ds > 0)]
            if not filt:
                return []
            # Î”Size ì˜¤ë¦„ì°¨ìˆœ, ê°™ì€ í¬ê¸°ë©´ Î”QoE ë‚´ë¦¼ì°¨ìˆœ
            filt.sort(key=lambda x: (x[1], -x[0]))
            # íŒŒë ˆí†  ì „ì„ : Î”Sizeê°€ ì‘ìœ¼ë©´ì„œ Î”QoEê°€ í° ê²ƒë§Œ ì±„íƒ
            frontier, best_qoe = [], -math.inf
            for dq, ds, ms in filt:
                if dq > best_qoe:
                    frontier.append((dq, ds, ms))
                    best_qoe = dq
            return frontier

        def _strict_slots_for(v):
            """ë¹„ë””ì˜¤ vê°€ ì—„ê²© ë°ë“œë¼ì¸ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìŠ¬ë¡¯ ì¸ë±ìŠ¤ë“¤."""
            d = int(self.deadline[v])
            return [i for i in range(self.n_slots) if (i % self.n_deadline) == d]

        # ë­í‚¹(ì •ë ¬/ì„ íƒ)ì— ì‚¬ìš©í•  ë¶„í¬ ì„ íƒ
        use_rank = "pred" if mode == "mckp" else "true"

        # ------------------ (0) ì´ˆê¸°í™”: ver0 ì„ í• ë‹¹ ------------------
        time_used = np.zeros(self.n_slots, dtype=np.float32)
        allocation_dict = {i: [] for i in range(self.n_slots)}
        current_combo = {v: {0} for v in range(self.n_videos)}       # í˜„ì¬ ë²„ì „ ì§‘í•©(ì´ˆê¸° {0})
        current_slot  = {}                                            # v -> slot

        # baseline QoE (í‰ê°€/ì •ë ¬ ê°ê°)
        baseline_qoe_eval = np.zeros(self.n_videos, dtype=np.float64)   # true ê¸°ì¤€
        baseline_qoe_rank = np.zeros(self.n_videos, dtype=np.float64)   # use_rank ê¸°ì¤€
        # í˜„ì¬ê¹Œì§€ ì±„íƒëœ Î”QoE(ë­í‚¹ ê¸°ì¤€)
        current_delta_rank = np.zeros(self.n_videos, dtype=np.float64)

        # ver0ë¥¼ strict deadline ì•ˆì—ì„œ load balancing(ìƒëŒ€ ì‚¬ìš©ë¥  ë‚®ì€ ê³³)
        for v in range(self.n_videos):
            size0 = float(self.time_size[v, 0])
            slots = _strict_slots_for(v)
            feasible = [i for i in slots if time_used[i] + size0 <= self.time_limit[i]]
            if feasible:
                def usage_ratio(i): return float(time_used[i] / (self.time_limit[i] + eps))
                i_sel = min(feasible, key=usage_ratio)
                time_used[i_sel] += size0
                allocation_dict[i_sel].append((v, 0))
                current_slot[v] = i_sel
            else:
                current_slot[v] = None  # ver0ì¡°ì°¨ ë¶ˆê°€í•œ ê·¹ë‹¨ ìƒí™©

            baseline_qoe_eval[v] = _qoe_for_combo(v, {0}, use="true")
            baseline_qoe_rank[v] = _qoe_for_combo(v, {0}, use=use_rank)

        # ------------------ (1) ë¹„ë””ì˜¤ë³„ í›„ë³´ ìƒì„±/í”„ë£¨ë‹ ------------------
        per_video_frontier = {}   # v -> list of (Î”QoE_rank, Î”Size, mask_set)
        for v in range(self.n_videos):
            cand = []
            # {ver1..ver6}ì˜ ëª¨ë“  ì¡°í•©(0~2^(n_ver-1)-1). mask==0ì€ baselineì´ë¯€ë¡œ ì œì™¸.
            for mask in range(1, 1 << (self.n_ver - 1)):
                vs = {0}
                for b in range(self.n_ver - 1):  # ë²„ì „1..6
                    if (mask >> b) & 1:
                        vs.add(b + 1)

                size_v = _size_for_combo(v, vs)
                size_0 = float(self.time_size[v, 0])
                delta_size = size_v - size_0
                if delta_size <= 0:
                    continue

                # ë­í‚¹ ê¸°ì¤€ Î”QoE
                qoe_rank = _qoe_for_combo(v, vs, use=use_rank)
                delta_qoe_rank = qoe_rank - baseline_qoe_rank[v]
                cand.append((delta_qoe_rank, delta_size, frozenset(vs)))

            per_video_frontier[v] = _pareto_prune(cand)

        # ------------------ (2) ì „ì—­ í›„ë³´ ì •ë ¬ (score=Î”QoE/Î”Size) ------------------
        global_cands = []
        for v, items in per_video_frontier.items():
            for dq, ds, s in items:
                score = dq / (ds + eps)
                global_cands.append((score, v, dq, ds, s))
        global_cands.sort(reverse=True, key=lambda x: x[0])

        # í˜„ì¬ ì¡°í•© ì‚¬ì´ì¦ˆ ìºì‹œ
        cur_size = {v: _size_for_combo(v, current_combo[v]) for v in range(self.n_videos)}

        # ------------------ (3) ë†’ì€ scoreë¶€í„° ì ìš© (ì•µì»¤/ì¬ì•µì»¤) ------------------
        for score, v, dq_rank, ds, new_set in global_cands:
            # ë­í‚¹ ê¸°ì¤€ QoE ê°œì„  ì—†ìœ¼ë©´ skip
            if dq_rank <= current_delta_rank[v] + 1e-12:
                continue

            new_total_size = _size_for_combo(v, new_set)
            old_total_size = cur_size[v]
            extra_needed = new_total_size - old_total_size

            anchor = current_slot.get(v, None)
            placed = False

            def _apply_to_slot(slot_idx):
                """vì˜ ê¸°ì¡´ ë°°ì¹˜ë¥¼ slot_idxë¡œ êµì²´."""
                nonlocal placed, time_used, allocation_dict
                # ê¸°ì¡´ ì•µì»¤ì—ì„œ ì œê±°
                if current_slot[v] is not None:
                    old_slot = current_slot[v]
                    allocation_dict[old_slot] = [(vid, ver) for (vid, ver) in allocation_dict[old_slot] if vid != v]
                    time_used[old_slot] -= old_total_size
                    time_used[old_slot] = max(0.0, time_used[old_slot])

                # ìƒˆ ìŠ¬ë¡¯ì— ìƒˆ ì¡°í•© ì¶”ê°€
                for ver in sorted(new_set):
                    allocation_dict[slot_idx].append((v, ver))
                time_used[slot_idx] += new_total_size

                # ìƒíƒœ ê°±ì‹ 
                current_slot[v] = slot_idx
                current_combo[v] = set(new_set)
                cur_size[v] = new_total_size
                current_delta_rank[v] = dq_rank
                placed = True

            # 3-1) in-place ì—…ê·¸ë ˆì´ë“œ
            if anchor is not None:
                if extra_needed <= 0:
                    _apply_to_slot(anchor)
                else:
                    if time_used[anchor] + extra_needed <= self.time_limit[anchor]:
                        _apply_to_slot(anchor)

            # 3-2) ì¬ì•µì»¤: strict ë°ë“œë¼ì¸ ë‚´ worst-fit
            if not placed:
                strict_slots = _strict_slots_for(v)
                feasible = [i for i in strict_slots if time_used[i] + new_total_size <= self.time_limit[i]]
                if feasible:
                    def leftover_after(i): return float(self.time_limit[i] - (time_used[i] + new_total_size))
                    i_sel = max(feasible, key=leftover_after)  # worst-fit
                    _apply_to_slot(i_sel)

            # 3-3) ì—¬ì „íˆ ë¶ˆê°€ë©´ skip

        # ------------------ (4) ìµœì¢… QoE ê³„ì‚°(í‰ê°€=TRUE) ------------------
        total_reward = 0.0
        for v in range(self.n_videos):
            # vì— ëŒ€í•´ ì‹¤ì œ ì €ì¥ëœ ë²„ì „ ì§‘í•© ìˆ˜ì§‘
            allocated_versions = set()
            for i in range(self.n_slots):
                for (vid, ver) in allocation_dict[i]:
                    if vid == v:
                        allocated_versions.add(ver)

            # fallback ë§µ
            fallback_versions = []
            for ver in range(self.n_ver):
                if ver in allocated_versions:
                    fallback_versions.append(ver)
                else:
                    lower = [vv for vv in allocated_versions if vv < ver]
                    fallback_versions.append(max(lower) if lower else 0)

            version_popularities = self.true_popularity[v]  # í‰ê°€(oracle)ìš©
            version_vmaf = self.vmaf[v, fallback_versions]
            reward = np.sum(version_popularities * version_vmaf) * 100.0
            total_reward += float(reward)

        scaled_reward = (total_reward / (self.expected_total_reward + eps)) * 10.0
        if verbose:
            tag = "MCKP" if mode == "mckp" else "ORACLE"
            #print(f"âœ¨ Combo-Î”Greedy({tag}) ì´ QoE (ìŠ¤ì¼€ì¼): {scaled_reward:.2f}")
        """
        # ------------------ (5) ì¬ê³„ì‚° ê¸°ë°˜ ê²€ì¦ ì¶œë ¥ ------------------
        re_time_used = np.zeros(self.n_slots, dtype=np.float32)
        for i in range(self.n_slots):
            if allocation_dict[i]:
                re_time_used[i] = sum(float(self.time_size[vid, ver]) for (vid, ver) in allocation_dict[i])

        overflow = np.where(re_time_used > self.time_limit + 1e-6)[0]
        if verbose:
            print("OVERFLOW slots:", overflow.tolist(), flush=True)
            print("max util:", float(np.max(re_time_used / (self.time_limit + 1e-9))), flush=True)

        # strict deadline ìœ„ë°˜ ìŠ¤ìº”: i%dead != deadline[vid] ì´ë©´ ìœ„ë°˜
        violations = []
        for i in range(self.n_slots):
            for (vid, ver) in allocation_dict[i]:
                slot_group = i % self.n_deadline
                if slot_group != int(self.deadline[vid]):
                    violations.append((i, int(vid), int(ver), int(self.deadline[vid]), int(slot_group)))

        if verbose:
            if violations:
                print(f"DEADLINE VIOLATIONS: {len(violations)} found", flush=True)
                print("  examples (slot, vid, ver, deadline[vid], slot_group):", violations[:10], flush=True)
            else:
                print("DEADLINE VIOLATIONS: none", flush=True)
        """
        return scaled_reward, allocation_dict



    

    def greedy_allocation_HUTF_strictDL(self):
        time_used = np.zeros(self.n_slots, dtype=np.float32)
        allocation_dict = {i: [] for i in range(self.n_slots)}
        assigned = np.zeros((self.n_videos, self.n_ver), dtype=bool)

        # (0-1) lowest version ì„ í• ë‹¹ (ver=0), load balancing + strict deadline
        lowest_slot = {}
        for v in range(self.n_videos):
            size = self.time_size[v, 0]
            candidate_slots = [
                i for i in range(self.n_slots)
                if self.deadline[v] == (i % self.n_deadline) and time_used[i] + size <= self.time_limit[i]
            ]
            if candidate_slots:
                idx = min(candidate_slots, key=lambda i: time_used[i] / (self.time_limit[i] + 1e-6))
                time_used[idx] += size
                allocation_dict[idx].append((v, 0))
                assigned[v, 0] = True
                lowest_slot[v] = idx
        
        # (1) score ê³„ì‚° ë° ì •ë ¬
        score_list = []
        for v in range(self.n_videos):
            for ver in range(1, self.n_ver):
                #score = (self.popularity[v, ver] * self.vmaf[v, ver]) / (self.time_size[v, ver] + 1e-6)
                score = (self.pred_popularity[v, ver] * self.vmaf[v, ver]) / (self.time_size[v, ver] + 1e-6)
                score_list.append((score, v, ver))
        score_list.sort(reverse=True)

        # (2) ë¹„ë””ì˜¤ë³„ ìµœì´ˆ ë“±ì¥ verê³¼ ì´í›„ ì²˜ë¦¬
        slot_per_video = {}

        for _, v, ver in score_list:
            if assigned[v, ver]:
                continue
            size = self.time_size[v, ver]

            if v not in slot_per_video:
                lowest_size = self.time_size[v, 0]


                #'''
                # here
                candidates = [
                    i for i in range(self.n_slots)
                    if self.deadline[v] == (i % self.n_deadline) and \
                    time_used[i] + lowest_size + size <= self.time_limit[i]
                ]

                if candidates:
                    # í›„ë³´ ì¤‘ í˜„ì¬ ì±„ì›€ ë¹„ìœ¨ì´ ê°€ì¥ ë‚®ì€ ìŠ¬ë¡¯ ì„ íƒ
                    idx = min(candidates, key=lambda i: time_used[i] / (self.time_limit[i] + 1e-6))

                    if v in lowest_slot:
                        prev = lowest_slot[v]
                        time_used[prev] -= self.time_size[v, 0]
                        allocation_dict[prev] = [item for item in allocation_dict[prev] if not (item[0] == v and item[1] == 0)]
                        assigned[v, 0] = False

                    time_used[idx] += lowest_size + size
                    allocation_dict[idx].append((v, 0))
                    allocation_dict[idx].append((v, ver))
                    assigned[v, 0] = True
                    assigned[v, ver] = True
                    slot_per_video[v] = idx
                #'''


                '''
                for i in range(self.n_slots):
                    if self.deadline[v] == (i % self.n_deadline) and \
                    time_used[i] + lowest_size + size <= self.time_limit[i]:
                        if v in lowest_slot:
                            prev = lowest_slot[v]
                            time_used[prev] -= self.time_size[v, 0]
                            allocation_dict[prev] = [item for item in allocation_dict[prev] if not (item[0] == v and item[1] == 0)]
                            assigned[v, 0] = False
                        time_used[i] += lowest_size + size
                        allocation_dict[i].append((v, 0))
                        allocation_dict[i].append((v, ver))
                        assigned[v, 0] = True
                        assigned[v, ver] = True
                        slot_per_video[v] = i
                        break
                '''
            else:
                i = slot_per_video[v]
                if time_used[i] + size <= self.time_limit[i]:
                    time_used[i] += size
                    allocation_dict[i].append((v, ver))
                    assigned[v, ver] = True
        #print(np.sum(time_used))
        #exit()

        total_reward = 0.0
        for v in range(self.n_videos):
            allocated_versions = [ver for i in range(self.n_slots)
                                  for (vid, ver) in allocation_dict[i] if vid == v]
            fallback_versions = []
            for ver in range(self.n_ver):
                if ver in allocated_versions:
                    fallback_versions.append(ver)
                else:
                    lower = [v2 for v2 in allocated_versions if v2 < ver]
                    fallback_versions.append(max(lower) if lower else 0)

            #version_popularities = self.popularity[v]
            version_popularities = self.true_popularity[v]
            version_vmaf = self.vmaf[v, fallback_versions]
            reward = np.sum(version_popularities * version_vmaf) * 100
            total_reward += reward

        scaled_reward = (total_reward / (self.expected_total_reward + 1e-6)) * 10.0
        #print(f"âœ¨ ê·¸ë¦¬ë””(HUTF-StrictDL) ì´ QoE (ìŠ¤ì¼€ì¼ëœ): {scaled_reward:.2f}")
        # self._log_util_stats(time_used, "Greedy HUTF-StrictDL")
        return scaled_reward, allocation_dict

    def EDF_allocation(self):
        # ì´ˆê¸°í™”
        time_used = np.zeros(self.n_slots, dtype=np.float32)
        allocation_dict = {i: [] for i in range(self.n_slots)}
        assigned = np.zeros((self.n_videos, self.n_ver), dtype=bool)

        # (0-1) ver=0 ì„ í• ë‹¹: ë°˜ë“œì‹œ "ë°ë“œë¼ì¸ ì´í•˜ ìŠ¬ë¡¯" ì¤‘ì—ì„œ
        for v in range(self.n_videos):
            size = self.time_size[v, 0]
            feasible_slots = [
                i for i in range(self.n_slots)
                if (i % self.n_deadline) <= self.deadline[v] and
                time_used[i] + size <= self.time_limit[i]
            ]
            if feasible_slots:
                # ê°€ì¥ ì—¬ìœ  ìˆëŠ” ìŠ¬ë¡¯ ì„ íƒ
                idx = min(feasible_slots, key=lambda i: time_used[i] / (self.time_limit[i] + 1e-6))
                time_used[idx] += size
                allocation_dict[idx].append((v, 0))
                assigned[v, 0] = True
            # else: ver=0ì¡°ì°¨ ë°ë“œë¼ì¸ ë‚´ì— ëª» ë„£ìœ¼ë©´ ì´ ë¹„ë””ì˜¤ëŠ” ë¯¸ìŠ¤ ê°€ëŠ¥

        # (1) EDF ìˆœì„œ
        video_order = sorted(range(self.n_videos), key=lambda v: self.deadline[v])

        # (2) ê° ë¹„ë””ì˜¤ì— ëŒ€í•´ ì¶”ê°€ ë²„ì „ í• ë‹¹ (ë°ë“œë¼ì¸ ì´í•˜ ìŠ¬ë¡¯ë§Œ)
        for v in video_order:
            for ver in range(1, self.n_ver):
                if assigned[v, ver]:
                    continue
                size = self.time_size[v, ver]
                for i in range(self.n_slots):
                    slot_deadline = i % self.n_deadline
                    if slot_deadline <= self.deadline[v] and time_used[i] + size <= self.time_limit[i]:
                        time_used[i] += size
                        allocation_dict[i].append((v, ver))
                        assigned[v, ver] = True
                        break

        # (3) QoE ê³„ì‚°: "ì ‘ê·¼ ê°€ëŠ¥í•œ ìŠ¬ë¡¯(ìŠ¬ë¡¯ê·¸ë£¹ â‰¤ ë¹„ë””ì˜¤ ë°ë“œë¼ì¸)ì— ì €ì¥ëœ ë²„ì „ë§Œ" ì‚¬ìš© ê°€ëŠ¥
        total_reward = 0.0
        for v in range(self.n_videos):
            # ë°ë“œë¼ì¸ ë‚´ ìŠ¬ë¡¯ì— ì‹¤ì œ ì €ì¥ëœ ë²„ì „ ì§‘í•©
            accessible_versions = set()
            for i in range(self.n_slots):
                if (i % self.n_deadline) <= self.deadline[v]:
                    for (vid, ver) in allocation_dict[i]:
                        if vid == v:
                            accessible_versions.add(ver)

            for ver in range(self.n_ver):
                if ver in accessible_versions:
                    used_ver = ver
                else:
                    lower = [vv for vv in accessible_versions if vv < ver]
                    used_ver = max(lower) if lower else None

                if used_ver is None:
                    # ë°ë“œë¼ì¸ ë‚´ì— ì–´ë–¤ ë²„ì „ë„ ì—†ìœ¼ë©´ ë¯¸ìŠ¤(ë³´ìˆ˜ì ìœ¼ë¡œ 0 QoE)
                    qoe = 0.0
                else:
                    qoe = self.vmaf[v, used_ver]

                total_reward += self.true_popularity[v, ver] * qoe

        total_reward *= 100.0
        normalized_reward = (total_reward / (self.expected_total_reward + 1e-6)) * 10.0
        print(f"EDF ì´ QoE (ìŠ¤ì¼€ì¼ëœ, strict deadline): {normalized_reward:.2f}")
        # self._log_util_stats(time_used, "EDF-StrictDL")
        return normalized_reward, allocation_dict

    def compute_final_qoe(self, allocation_dict):
        """
        í˜„ì¬ ì—í”¼ì†Œë“œì˜ ìµœì¢… ë°°ì¹˜ë§Œìœ¼ë¡œ QoE ì¬ê³„ì‚°.
        - full_qoe: ëª¨ë“  ìš”ì²­ì´ ì›í•˜ëŠ” ë²„ì „ ê·¸ëŒ€ë¡œ ì œê³µëë‹¤ê³  ê°€ì •(= expected_total_reward)
        - actual_qoe: ì €ì¥ëœ ë²„ì „ + fallback ê·œì¹™ìœ¼ë¡œ ì œê³µë  ë•Œì˜ QoE
        ë°˜í™˜ì€ (actual_qoe, full_qoe), ê·¸ë¦¬ê³  ë¡œê·¸ë¡œ ë¹„ìœ¨/ìŠ¤ì¼€ì¼ ì¶œë ¥.
        """
        # 1) ë¹„ë””ì˜¤ë³„ ì €ì¥ëœ ë²„ì „ ì§‘í•© í†µí•©(ì¤‘ë³µ/ì´ë™ ì •ë¦¬)
        saved_by_video = [set() for _ in range(self.n_videos)]
        for slot in range(self.n_slots):
            for (vid, vers) in allocation_dict.get(slot, []):
                # vers ê°€ [0, 2, 5] ê°™ì€ ë¦¬ìŠ¤íŠ¸ë¡œ ë“¤ì–´ì˜¤ë¯€ë¡œ set ì—…ë°ì´íŠ¸
                if isinstance(vers, (list, tuple, np.ndarray)):
                    saved_by_video[vid].update(int(v) for v in vers)
                else:
                    saved_by_video[vid].add(int(vers))

        # 2) fallback ê·œì¹™
        def served_version(saved_set, req_ver):
            if req_ver in saved_set:
                return req_ver
            lowers = [v for v in saved_set if v < req_ver]
            return max(lowers) if lowers else 0

        # 3) ì‹¤ì œ QoE í•©ì‚°(actual) / ì´ìƒì  QoE(full)
        actual_qoe = 0.0
        for v in range(self.n_videos):
            saved = saved_by_video[v]
            for ver in range(self.n_ver):
                use_ver = served_version(saved, ver)
                actual_qoe += float(self.true_popularity[v, ver] * self.vmaf[v, use_ver])

        full_qoe = float(self.expected_total_reward)  # ì´ë¯¸ ë™ì¼ ì •ì˜

        # 4) ì§€í‘œ ì¶œë ¥
        ratio = (actual_qoe / (full_qoe + 1e-6)) * 100.0
        scaled = (actual_qoe * 1000.0) / (full_qoe + 1e-6)   # í•™ìŠµì‹œ ìŠ¤ì¼€ì¼ê³¼ ë™ì¼(â‰ˆ 984 ë“±)
        #print(f"ğŸ”® ì œê³µëœ ì´ QoE / ê¸°ëŒ€ QoE: {ratio:.2f}% (ìŠ¤ì¼€ì¼={scaled:.2f})")

        return actual_qoe, full_qoe


class RewardLoggerCallback(BaseCallback):
    def __init__(self, verbose=1, num_envs=4):  # ğŸ”¹ num_envs ì¶”ê°€
        super().__init__(verbose)
        self.episode_rewards = []
        self.current_episode_reward = np.zeros(num_envs)  # ğŸ”¹ ê° í™˜ê²½ë³„ ë³´ìƒ ì €ì¥

    def _on_step(self):
        # âœ… í˜„ì¬ stepì—ì„œ ë°œìƒí•œ rewardë¥¼ í™˜ê²½ë³„ë¡œ ëˆ„ì 
        if "rewards" in self.locals:
            self.current_episode_reward += self.locals["rewards"]  # ğŸ”¹ ê°œë³„ í™˜ê²½ë³„ ë¦¬ì›Œë“œ ì €ì¥

        # âœ… ì—í”¼ì†Œë“œê°€ ì¢…ë£Œë  ë•Œ (ê° í™˜ê²½ë³„ ê°œë³„ ì €ì¥)
        if "dones" in self.locals:
            for i, done in enumerate(self.locals["dones"]):  # ğŸ”¹ ê° í™˜ê²½ë³„ ì¢…ë£Œ í™•ì¸
                if done:
                    self.episode_rewards.append(self.current_episode_reward[i])  # ê°œë³„ í™˜ê²½ì˜ ë³´ìƒ ì €ì¥
                    self.current_episode_reward[i] = 0  # ğŸ”¹ ìƒˆë¡œìš´ ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ í•´ë‹¹ í™˜ê²½ ì´ˆê¸°í™”

        return True  # ê³„ì† í•™ìŠµ ì§„í–‰

if __name__ == "__main__":

    def make_env(video_model="logit_blend", version_model="dirichlet",
             video_param=0.03, video_tau=0.10, video_lambda=0.40,
             ver_param=0.25, seed=None):
        def _init():
            env = TransEnv(
                video_noise_model=video_model,
                video_noise_param=video_param,
                video_tau=video_tau,
                video_lambda=video_lambda,
                version_noise_model=version_model,
                version_noise_param=ver_param,
                seed=seed
            )
            return ActionMasker(env, lambda e: e.get_valid_action_mask())
        return _init


    # ----------------- í•˜ì´í¼íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¤„ -----------------
    lr0 = 3e-4
    def lr_schedule(p):         # p: 1 -> 0
        return lr0 * (0.3 + 0.7 * p)

    def ent_schedule(p):        # ì´ˆë°˜ 0.022 -> í›„ë°˜ 0.002
        return 0.02 * p + 0.002

    # ----------------- í™˜ê²½ ìƒì„± -----------------
    '''
    env = SubprocVecEnv([
        make_env(VIDEO_TRAINING_MODEL, VER_TRAINING_MODEL,
                video_param=VIDEO_PARAM, video_tau=VIDEO_TAU, video_lambda=VIDEO_LAMBDA,
                ver_param=VER_PARAM, seed=100+i)
        for i in range(num_envs)
    ])
    '''

    env = DummyVecEnv([
        make_env(VIDEO_TRAINING_MODEL, VER_TRAINING_MODEL,
                video_param=VIDEO_PARAM, video_tau=VIDEO_TAU, video_lambda=VIDEO_LAMBDA,
                ver_param=VER_PARAM, seed=100+i)
        for i in range(num_envs)
    ])


    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=5.0)  # âœ… ê´€ì¸¡ ì •ê·œí™”

    policy_kwargs = dict(
        net_arch=dict(pi=[256, 256], vf=[256, 256]),
        activation_fn=torch.nn.SiLU,   # or Tanh
        ortho_init=False               # í° ë„¤íŠ¸ì›Œí¬ë©´ Falseê°€ ì‹¤ì „ì—ì„œ ì¢…ì¢… ì•ˆì •ì 
    )

    # ----------------- ëª¨ë¸ êµ¬ì„± -----------------
    model = MaskablePPO(
        "MlpPolicy",
        env,
        policy_kwargs=policy_kwargs,
        device=device,
        verbose=1,
        n_steps=2048,          # âœ… 4096 -> 1024
        batch_size=2048,
        n_epochs=12,           # âœ… 20 -> 12
        learning_rate=lr_schedule,
        ent_coef=0.018, # âœ… ìŠ¤ì¼€ì¤„
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.25,
        vf_coef=0.5,
        max_grad_norm=0.5,
        target_kl=0.02         # âœ… 0.01 -> 0.02 (ì—…ë°ì´íŠ¸ í­ ì¡°ê¸ˆ í—ˆìš©)
    )

    reward_callback = RewardLoggerCallback()

    if TRAINING:
        if os.path.exists(MODEL_PATH):
            print(f"âœ… ê¸°ì¡´ ëª¨ë¸ {MODEL_PATH} ë¶ˆëŸ¬ì˜¤ê¸°...")
            if FINETUNE:
                model = MaskablePPO.load(
                MODEL_PATH, env=env, device=device,
                custom_objects={
                    "ent_coef": 0.0,          # íƒìƒ‰ ë„ê¸°
                    "learning_rate": 1e-5,    # ì‘ì€ í•™ìŠµë¥ 
                    "clip_range": 0.1         # ê³¼ë„í•œ ì—…ë°ì´íŠ¸ ë°©ì§€
                }
            )
                print("ğŸ”§ ê²°ì •ë¡  í…ŒìŠ¤íŠ¸ ëŒ€ë¹„ ë¯¸ì„¸ í•™ìŠµ ì‹œì‘...")
                model.learn(total_timesteps=FINETUNE_STEPS, log_interval=10, callback=reward_callback)
            else:
                model = MaskablePPO.load(MODEL_PATH, env=env, device=device)
                model.learn(total_timesteps=1000000, log_interval=10, callback=reward_callback)
    
        else:
            model.learn(total_timesteps=TOTAL_STEPS, log_interval=10, callback=reward_callback)
   
        # âœ… ì €ì¥
        model.save(MODEL_PATH)
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {MODEL_PATH}")
        env.save("vecnorm.pkl")   # âœ… VecNormalize í†µê³„ ì €ì¥

        # âœ… ë¦¬ì›Œë“œ ê·¸ë˜í”„ ì¶œë ¥
        plt.figure(figsize=(10, 5))
        plt.plot(reward_callback.episode_rewards, label="Total Reward per Episode")
        plt.xlabel("Episodes")
        plt.ylabel("Total Reward")
        plt.title("Learning Curve: PPO Training Progress")
        plt.legend()
        plt.grid()
        plt.show()



    def export_plan(env, policy_name="DRL", alloc=None, out_dir="plans"):
        """
        env: test_env.envs[0].unwrapped (TransEnv)
        policy_name: "DRL" / "HUF" / "HUTF" / ...
        alloc: ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì§ì ‘ ë¦¬í„´í•œ allocation_dict (ì—†ìœ¼ë©´ env.allocation_dict ì‚¬ìš©)
        """
        import os, json, datetime

        if alloc is None:
            alloc = env.allocation_dict   # DRL ê²½ìš° ê¸°ë³¸ê°’

        n_slots = env.n_slots
        n_deadlines = env.n_deadline
        n_servers = n_slots // n_deadlines
        time_limits = env.time_limit.tolist()

        ladder = [
            {"ver": 0, "width": 256,  "height": 144,  "label": "144p"},
            {"ver": 1, "width": 320,  "height": 240,  "label": "240p"},
            {"ver": 2, "width": 384,  "height": 288,  "label": "288p"},
            {"ver": 3, "width": 480,  "height": 360,  "label": "360p"},
            {"ver": 4, "width": 640,  "height": 480,  "label": "480p"},
            {"ver": 5, "width": 1280, "height": 720,  "label": "720p"},
            {"ver": 6, "width": 1920, "height": 1080, "label": "1080p"},
        ]

        slots = {}
        for s in range(n_slots):
            jobs_by_vid = {}
            for (vid0, ver_or_list) in alloc.get(s, []):
                vid = int(vid0) + 1
                # DRL: ver_or_list = [0,1,2] / HUF: ver_or_list = 2
                vers = ver_or_list if isinstance(ver_or_list, (list, tuple)) else [ver_or_list]
                if vid not in jobs_by_vid:
                    jobs_by_vid[vid] = {"video_id": vid, "video": f"{vid}.mp4", "versions": []}
                jobs_by_vid[vid]["versions"].extend(int(v) for v in vers)
            # ì¤‘ë³µ ë²„ì „ ì œê±° + ì •ë ¬
            for job in jobs_by_vid.values():
                job["versions"] = sorted(set(job["versions"]))
            slots[str(s)] = list(jobs_by_vid.values())

        plan = {
            "policy": policy_name,
            "plan_id": f"{policy_name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "n_slots": n_slots,
            "n_deadlines": n_deadlines,
            "n_servers": n_servers,
            "ladder": ladder,
            "time_limits": {str(i): float(t) for i, t in enumerate(time_limits)},
            "slots": slots
        }

        os.makedirs(out_dir, exist_ok=True)
        out_path = os.path.join(out_dir, f"{policy_name}.json")
        with open(out_path, "w") as f:
            json.dump(plan, f, indent=2)
        print(f"[INFO] plan exported -> {out_path}")






    if TESTING:

        num_test_episodes = 50
        results = []  # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸

        # SIZE_FACTORë¥¼ 0.01 ~ 0.99ê¹Œì§€ 0.01ì”© ì¦ê°€
        for SIZE_FACTOR in tqdm(np.arange(0.3, 0.85, 0.01), desc="SIZE FACTOR Progress"):
            print(f"\n===== SIZE_FACTOR = {SIZE_FACTOR:.2f} =====")

            # === í™˜ê²½ ìƒì„± ===
            test_env = DummyVecEnv([
                make_env(VIDEO_TRAINING_MODEL, VER_TRAINING_MODEL,
                        video_param=VIDEO_PARAM,
                        video_tau=VIDEO_TAU,
                        video_lambda=VIDEO_LAMBDA,
                        ver_param=VER_PARAM)
            ])
            test_env = VecNormalize.load("vecnorm.pkl", test_env)
            test_env.training = False
            test_env.norm_reward = False

            # === MCKP_predict baseline ê³„ì‚° ===
            MCKP_predict_QoE = 0
            for episode in tqdm(range(num_test_episodes), desc="MCKP_Episodes", leave=False):
                obs = test_env.reset()
                done = False
                while not done:
                    # action ì—†ì´ í™˜ê²½ì´ ê²°ì • (greedy baseline)
                    obs, reward, done, info = test_env.step([0])   # dummy action
                MCKP_predict_QoE += test_env.env_method("get_MCKP_predict_value")[0]

            MCKP_predict_val = (MCKP_predict_QoE / num_test_episodes) / 10

            # === DRL & Baseline_DRL ===
            drl_results = {}

            for model_name, model_path in MODEL_PATHS.items():
                PPO_QoE = 0

                model = MaskablePPO.load(model_path, env=test_env, device=device)

                for episode in tqdm(range(num_test_episodes), desc=f"{model_name} Episodes", leave=False):
                    obs = test_env.reset()
                    done = False
                    while not done:
                        mask = test_env.env_method('get_valid_action_mask')[0]
                        action, _ = model.predict(obs, action_masks=mask, deterministic=True)
                        obs, reward, done, info = test_env.step(action)

                    PPO_QoE += test_env.env_method("get_PPO_value")[0]

                drl_results[model_name] = (PPO_QoE / num_test_episodes) / 10

            # === ê²°ê³¼ ì €ì¥ ===
            results.append([
                SIZE_FACTOR,
                drl_results["DRL"],
                drl_results["Baseline_DRL"],
                MCKP_predict_val
            ])

        # === CSV íŒŒì¼ ì €ì¥ ===
        with open("Provisioning_Ratio.csv", "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([
                "SIZE_FACTOR", "PPO_DRL", "PPO_BASELINE", "MCKP_predict"
            ])
            writer.writerows(results)

        print("All saved to Provisioning_Ratio.csv")

        

        THRESHOLDS = [90, 95]
        algorithms = ["PPO_DRL", "PPO_BASELINE", "MCKP_predict"]

        df = pd.read_csv("Provisioning_Ratio.csv")

        pr_results = {alg: {} for alg in algorithms}

        for alg in algorithms:
            for th in THRESHOLDS:
                row = df[df[alg] >= th]
                if not row.empty:
                    pr_results[alg][f"PR_{int(th)}"] = float(row.iloc[0]["SIZE_FACTOR"])
                else:
                    pr_results[alg][f"PR_{int(th)}"] = None

        
        import json
        with open("PR_results.json", "w") as f:
            json.dump(pr_results, f, indent=4)

        print("ğŸ¯ PR extraction complete. Saved to PR_results.json")




        '''
        num_test_episodes = 50
        results = []  # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸

        # SIZE_FACTORë¥¼ 0.01 ~ 0.99ê¹Œì§€ 0.01ì”© ì¦ê°€
        for SIZE_FACTOR in tqdm(np.arange(0.01, 1.0, 0.01), desc="SIZE FACTOR Progress"):
            print(f"\n===== SIZE_FACTOR = {SIZE_FACTOR:.2f} =====")

            # === í™˜ê²½ ìƒì„± ===
            test_env = DummyVecEnv([
                make_env(VIDEO_TRAINING_MODEL, VER_TRAINING_MODEL,
                        video_param=VIDEO_PARAM,
                        video_tau=VIDEO_TAU,
                        video_lambda=VIDEO_LAMBDA,
                        ver_param=VER_PARAM)
            ])
            test_env = VecNormalize.load("vecnorm.pkl", test_env)
            test_env.training = False
            test_env.norm_reward = False


            # ê²°ê³¼ ë³€ìˆ˜ ì´ˆê¸°í™”
            PPO_QoE = HUF_QoE = HUTF_QoE = 0
            HUF_strict_QoE = HUTF_strict_QoE = 0
            MCKP_predict_QoE = MCKP_true_QoE = 0

            model = MaskablePPO.load(MODEL_PATH, env=test_env, device=device)

            # === ì—í”¼ì†Œë“œ ë°˜ë³µ ===
            for episode in tqdm(range(num_test_episodes), desc="Episode Progress", leave=False):
                #print(f"\nğŸš€ í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ {episode+1} ì‹œì‘")
                obs = test_env.reset()
                done = False
                while not done:
                    mask = test_env.env_method('get_valid_action_mask')[0]
                    action, _ = model.predict(obs, action_masks=mask, deterministic=True)
                    obs, reward, done, info = test_env.step(action)

                # QoE ê°’ ëˆ„ì 
                HUF_QoE          += test_env.env_method("get_HUF_value")[0]
                HUTF_QoE         += test_env.env_method("get_HUTF_value")[0]
                HUF_strict_QoE   += test_env.env_method("get_HUF_strict_value")[0]
                HUTF_strict_QoE  += test_env.env_method("get_HUTF_strict_value")[0]
                MCKP_predict_QoE += test_env.env_method("get_MCKP_predict_value")[0]
                MCKP_true_QoE    += test_env.env_method("get_MCKP_true_value")[0]
                PPO_QoE          += test_env.env_method("get_PPO_value")[0]

            # === í‰ê·  QoE ê³„ì‚° ===
            PPO_val          = (PPO_QoE / num_test_episodes) / 10
            HUF_val          = (HUF_QoE / num_test_episodes) / 10
            HUTF_val         = (HUTF_QoE / num_test_episodes) / 10
            HUF_strict_val   = (HUF_strict_QoE / num_test_episodes) / 10
            HUTF_strict_val  = (HUTF_strict_QoE / num_test_episodes) / 10
            MCKP_predict_val = (MCKP_predict_QoE / num_test_episodes) / 10
            MCKP_true_val    = (MCKP_true_QoE / num_test_episodes) / 10

            print("====decision results====")
            print("PPO value : ", PPO_val)
            print("HUF value : ", HUF_val)
            print("HUTF value : ", HUTF_val)
            print("HUF_strict value : ", HUF_strict_val)
            print("HUTF_strict value : ", HUTF_strict_val)
            print("MCKP_predict value : ", MCKP_predict_val)
            print("MCKP_true value : ", MCKP_true_val)

            # === ê²°ê³¼ ì €ì¥ ===
            results.append([
                SIZE_FACTOR, PPO_val, HUF_val, HUTF_val,
                HUF_strict_val, HUTF_strict_val,
                MCKP_predict_val, MCKP_true_val
            ])

        # === CSV íŒŒì¼ë¡œ ì €ì¥ ===
        with open("decision_results_hetero_baseline.csv", "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([
                "SIZE_FACTOR", "PPO", "HUF", "HUTF",
                "HUF_strict", "HUTF_strict",
                "MCKP_predict", "MCKP_true"
            ])
            writer.writerows(results)

        print("âœ… ëª¨ë“  ê²°ê³¼ decision_results.csv íŒŒì¼ì— ì €ì¥ ì™„ë£Œ!")

        '''





        '''


        num_test_episodes = 50  # í…ŒìŠ¤íŠ¸í•  ì—í”¼ì†Œë“œ ìˆ˜
        #num_test_episodes = 1  # í…ŒìŠ¤íŠ¸í•  ì—í”¼ì†Œë“œ ìˆ˜

        # DummyVecEnv
        test_env = DummyVecEnv([
            make_env(VIDEO_TRAINING_MODEL, VER_TRAINING_MODEL,
                    video_param=VIDEO_PARAM,
                    video_tau=VIDEO_TAU,
                    video_lambda=VIDEO_LAMBDA,
                    ver_param=VER_PARAM)
        ])

        test_env = VecNormalize.load("vecnorm.pkl", test_env)
        test_env.training = False
        test_env.norm_reward = False

        PPO_QoE = 0
        HUF_QoE = 0
        HUTF_QoE = 0
        HUF_strict_QoE = 0
        HUTF_strict_QoE = 0
        MCKP_predict_QoE = 0
        MCKP_true_QoE = 0

        model = MaskablePPO.load(MODEL_PATH, env=test_env, device=device)

        start = time.time()     # ì‹œì‘ ì‹œê°
        for episode in range(num_test_episodes):
            print(f"\nğŸš€ í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ {episode+1} ì‹œì‘")
            obs = test_env.reset()
            done = False
            while not done:
                mask = test_env.env_method('get_valid_action_mask')[0]  # ë‹¨ì¼ env
                action, _ = model.predict(obs, action_masks=mask, deterministic=True)
                #action, _ = model.predict(obs, action_masks=mask)
                obs, reward, done, info = test_env.step(action)
                

            
            HUF_QoE          += test_env.env_method("get_HUF_value")[0]
            HUTF_QoE         += test_env.env_method("get_HUTF_value")[0]
            HUF_strict_QoE   += test_env.env_method("get_HUF_strict_value")[0]
            HUTF_strict_QoE  += test_env.env_method("get_HUTF_strict_value")[0]
            MCKP_predict_QoE += test_env.env_method("get_MCKP_predict_value")[0]
            MCKP_true_QoE    += test_env.env_method("get_MCKP_true_value")[0]
            PPO_QoE          += test_env.env_method("get_PPO_value")[0]
            

        end = time.time()       # ì¢…ë£Œ ì‹œê°

        total_time = end - start
        avg_time = total_time / num_test_episodes
        print(f"ì—í”¼ì†Œë“œë‹¹ í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_time:.2f}ì´ˆ")
        



        
        print("====decision results====")
        print("PPO value : ", (PPO_QoE / num_test_episodes) / 10)
        print("HUF value : ", (HUF_QoE / num_test_episodes) / 10)
        print("HUTF value : ", (HUTF_QoE / num_test_episodes) / 10)
        print("HUF_strict value : ", (HUF_strict_QoE / num_test_episodes) / 10)
        print("HUTF_strict value : ", (HUTF_strict_QoE / num_test_episodes) / 10)
        print("MCKP_predict value : ", (MCKP_predict_QoE / num_test_episodes) / 10)
        print("MCKP_true value : ", (MCKP_true_QoE / num_test_episodes) / 10)

        '''

        '''
        env = test_env.envs[0].unwrapped
        #export_plan(env, "DRL", out_dir="plans")  # alloc ì¸ì ìƒëµ
        export_plan(env, "baseline_DRL", out_dir="plans")  # alloc ì¸ì ìƒëµ
        export_plan(env, "HUF", out_dir="plans", alloc=env.HUF_alloc)
        export_plan(env, "HUTF", out_dir="plans", alloc=env.HUTF_alloc)
        export_plan(env, "HUF_strictDL", out_dir="plans", alloc=env.HUF_strict_alloc)
        export_plan(env, "HUTF_strictDL", out_dir="plans", alloc=env.HUTF_strict_alloc)
        export_plan(env, "MCKP_predict", out_dir="plans", alloc=env.MCKP_predict_alloc)
        export_plan(env, "MCKP_true", out_dir="plans", alloc=env.MCKP_true_alloc)
        '''

        
